<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>fairseq的使用入门</title>
      <link href="/2022/08/29/fairseq/"/>
      <url>/2022/08/29/fairseq/</url>
      
        <content type="html"><![CDATA[<h2 id="fairseq常用命令"><a href="#fairseq常用命令" class="headerlink" title="fairseq常用命令"></a>fairseq常用命令</h2><h3 id="通用命名参数"><a href="#通用命名参数" class="headerlink" title="通用命名参数"></a>通用命名参数</h3><div class="table-container"><table><thead><tr><th style="text-align:left">命令</th><th style="text-align:left">参数</th><th style="text-align:left">用法</th><th style="text-align:left">可选项/备注</th></tr></thead><tbody><tr><td style="text-align:left">通用命名参数</td><td style="text-align:left"><code>--no-progress-bar</code></td><td style="text-align:left">关闭进度条</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--log-interval</code></td><td style="text-align:left">每N批记录进度(禁用进度条时)</td><td style="text-align:left">默认：100</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--log-format</code></td><td style="text-align:left">选择日志格式</td><td style="text-align:left">选项：json,none,simple,tqdm</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--log-file</code></td><td style="text-align:left">指定输出metric的日志文件</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--tensorboard-logdir</code></td><td style="text-align:left">tensorboard的日志文件位置</td><td style="text-align:left">须和-–logdir of running tensorboard相同，默认无日志</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--wandb-project</code></td><td style="text-align:left">用于日志的权重和偏置项目名</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--seed</code></td><td style="text-align:left">设置随机数种子</td><td style="text-align:left">默认为1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--cpu</code></td><td style="text-align:left">使用CPU而不是CUDA</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--tpu</code></td><td style="text-align:left">使用TPU而不是CUDA</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--bf16</code></td><td style="text-align:left">使用数据格式bfloat16,前提—tpu</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--memory-efficient-bf16</code></td><td style="text-align:left">使用内存高效的bfloat16,前提—bf16</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--fp16</code></td><td style="text-align:left">使用数据格式FP16</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--memory-efficient-fp16</code></td><td style="text-align:left">使用数据格式内存高效的FP16,</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--fp16-no-flatten-grads</code></td><td style="text-align:left">不展开FP16梯度tensor</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--fp16-init-scale</code></td><td style="text-align:left"></td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--on-cpu-convert-precision</code></td><td style="text-align:left">浮点数转化为fp16/bf16将在CPU上完成.</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--amp</code></td><td style="text-align:left">使用自动混合精度</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--usr-dir</code></td><td style="text-align:left">包含个性扩展的模块路径（tasks/architectures）</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--empty-cache-freq</code></td><td style="text-align:left">pytorch CUDA缓存清理频率</td><td style="text-align:left">默认：0（不清理）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--model-parallel-size</code></td><td style="text-align:left">使用并行的GPU数量</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--quantization-config-path</code></td><td style="text-align:left">quantization配置文件路径</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--criterion</code></td><td style="text-align:left">损失函数</td><td style="text-align:left">默认：交叉熵</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--tokenizer</code></td><td style="text-align:left">分词工具</td><td style="text-align:left">可选：mosses,nltk,space</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--bpe</code></td><td style="text-align:left">bpe分词</td><td style="text-align:left">可选：byte_bpe, bytes, characters, fastbpe, gpt2, bert, hf_byte_bpe, sentencepiece, subword_nmt</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--optimizer</code></td><td style="text-align:left">优化器</td><td style="text-align:left">可选：adadelta, adafactor, adagrad, adam, adamax, composite, cpu_adam, lamb, nag, sgd</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--lr-scheduler</code></td><td style="text-align:left">学习率管理</td><td style="text-align:left">可选：cosine, fixed, inverse_sqrt, manual, pass_through, polynomial_decay, reduce_lr_on_plateau, step, tri_stage, triangular，默认：fixed</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--scoring</code></td><td style="text-align:left">评价指标</td><td style="text-align:left">可选：bert_score, sacrebleu, bleu, chrf, meteor, wer，默认：bleu</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--task</code></td><td style="text-align:left">任务选项</td><td style="text-align:left">可选：multilingual_language_modeling, speech_unit_modeling, hubert_pretraining, translation等，默认：translation</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--dataset-impl</code></td><td style="text-align:left">处理数据集方式</td><td style="text-align:left">可选：raw, lazy, cached, mmap, fasta, huffman，默认：mmap</td></tr></tbody></table></div><p>命名参数省略：</p><ul><li>aim-repo</li><li>aim-run-hash</li><li>azureml-logging</li><li>fp16-init-scale</li><li>fp16-scale-window</li><li>fp16-scale-tolerance</li><li>min-loss-scale</li><li>threshold-loss-scale</li><li>amp-batch-retries</li><li>amp-init-scale</li><li>amp-scale-window</li><li>all-gather-list-size</li><li>profile</li><li>reset-logging</li><li>suppress-crashes</li><li>use-plasma-view</li><li>plasma-path</li></ul><h3 id="fairseq-preprocess参数"><a href="#fairseq-preprocess参数" class="headerlink" title="fairseq-preprocess参数"></a>fairseq-preprocess参数</h3><div class="table-container"><table><thead><tr><th style="text-align:left">命令</th><th style="text-align:left">参数</th><th style="text-align:left">用法</th><th style="text-align:left">可选项/备注</th></tr></thead><tbody><tr><td style="text-align:left">fairseq-preprocess</td><td style="text-align:left"></td><td style="text-align:left">数据预处理：建词典并将训练数据二进制化</td><td style="text-align:left">以下为预处理命令</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>-s, --source-lang</code></td><td style="text-align:left">源语言</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>-t, --target-lang</code></td><td style="text-align:left">目标语言</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--trainpref</code></td><td style="text-align:left">两个语言的训练文件前缀</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--validpref</code></td><td style="text-align:left">两个语言的验证文件前缀</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--testpref</code></td><td style="text-align:left">两个语言的测试文件前缀</td><td style="text-align:left">说明：例如我们的数据中只有训练数据和测试数据，且文件后缀为src和tgt，即train.src、train.tgt、test.src和test.tgt，那么通过指定—source-lang src —target-lang tgt —trainpref train —testpref test，也可以读取的对应的文件。</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--align-suffix</code></td><td style="text-align:left">对齐后缀</td><td style="text-align:left">未知</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--destdir</code></td><td style="text-align:left">输出文件位置</td><td style="text-align:left">默认：data-bin</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--thresholdtgt</code></td><td style="text-align:left">将目标语言出现次数少于阈值的词映射到unkown</td><td style="text-align:left">默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--thresholdsrc</code></td><td style="text-align:left">将源语言出现次数少于阈值的词映射到unkown</td><td style="text-align:left">默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--tgtdict</code></td><td style="text-align:left">重复使用给定的目标语言字典</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--srcdict</code></td><td style="text-align:left">重复使用给定的源语言字典</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--nwordstgt</code></td><td style="text-align:left">目标语言中需要retain的词数量</td><td style="text-align:left">默认：-1（未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--nwordssrc</code></td><td style="text-align:left">源语言中需要retain的词数量</td><td style="text-align:left">默认：-1（未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--alignfile</code></td><td style="text-align:left">对齐文件（可选）</td><td style="text-align:left">未知</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--joined-dictionary</code></td><td style="text-align:left">生成联合的字典</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--only-source</code></td><td style="text-align:left">只处理源语言</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--padding-factor</code></td><td style="text-align:left">填充字典数量为N的倍数</td><td style="text-align:left">默认：8</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--workers</code></td><td style="text-align:left">并行进程数量</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--dict-only</code></td><td style="text-align:left">只构建字典</td><td style="text-align:left">默认：False</td></tr></tbody></table></div><h3 id="fairseq-train参数"><a href="#fairseq-train参数" class="headerlink" title="fairseq-train参数"></a>fairseq-train参数</h3><div class="table-container"><table><thead><tr><th style="text-align:left">命令</th><th style="text-align:left">参数</th><th style="text-align:left">用法</th><th style="text-align:left">可选项/备注</th></tr></thead><tbody><tr><td style="text-align:left">fairseq-train</td><td style="text-align:left"></td><td style="text-align:left">在一个或多个GPU上训练模型</td><td style="text-align:left">以下为训练命令</td></tr><tr><td style="text-align:left">数据集加载</td><td style="text-align:left"><code>--num-workers</code></td><td style="text-align:left">加载数据的并行进程数量</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--skip-invalid-size-inputs-valid-test</code></td><td style="text-align:left">忽略验证集和测试集中太长或太短的句子</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--max-token</code></td><td style="text-align:left">一个batch中tokens的最大数量</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--batch-size,--max-sentences</code></td><td style="text-align:left">batch size</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--required-batch-size-multiple</code></td><td style="text-align:left">要求batchsize是该值的倍数</td><td style="text-align:left">默认：8</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--required-seq-len-multiple</code></td><td style="text-align:left">要求最大序列长度是该值的倍数</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--data-buffer-size</code></td><td style="text-align:left">预先加载的batch数量</td><td style="text-align:left">default：10</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--train-subset</code></td><td style="text-align:left">数据集中用于训练的子集</td><td style="text-align:left">例如：trian,valid,test。默认：train</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--valid-subset</code></td><td style="text-align:left">数据集中用于验证的子集</td><td style="text-align:left">例如：trian,valid,test。默认：valid</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--ignore-unused-valid-subsets</code></td><td style="text-align:left">do not raise error if valid subsets are ignored</td><td style="text-align:left">默认：False（未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--validate-interval</code></td><td style="text-align:left">每N个epochs在验证集上验证</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--validate-interval-updates</code></td><td style="text-align:left">每N个updates在验证集上验证</td><td style="text-align:left">默认：0 （未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--validate-after-updates</code></td><td style="text-align:left">在N次更新后才验证</td><td style="text-align:left">默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--fixed-validation-seed</code></td><td style="text-align:left">设置验证时的种子值</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--disable-validation</code></td><td style="text-align:left">取消验证</td><td style="text-align:left">default:False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--max-tokens-valid</code></td><td style="text-align:left">验证时每批次的最大tokens数</td><td style="text-align:left">默认和 <code>--max-tokens</code>相同</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--batch-size-valid</code></td><td style="text-align:left">验证时batchsize</td><td style="text-align:left">默认和<code>--batch-size</code>相同</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--max-valid-steps，--nval</code></td><td style="text-align:left">评估的总batch数量</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--curriculum</code></td><td style="text-align:left">在前N个epochs中不打乱batch</td><td style="text-align:left">N默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--gen-subset</code></td><td style="text-align:left">选择生成的数据集子集（train,valid,text）</td><td style="text-align:left">默认：test</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--num-shards</code></td><td style="text-align:left">shard generation over N shards</td><td style="text-align:left">默认：1 （未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--shard-id</code></td><td style="text-align:left">生成shards的其中第i个（i&lt;num-shards）</td><td style="text-align:left">默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--grouped-shuffling</code></td><td style="text-align:left">在num_shards组中打乱批次，使得每个子进程中的序列长度相似，前提batches按照长度排序</td><td style="text-align:left">默认：False （未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--update-epoch-batch-itr</code></td><td style="text-align:left"></td><td style="text-align:left">默认：False （未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--update-ordered-indices-seed</code></td><td style="text-align:left"></td><td style="text-align:left">默认：False （未知）</td></tr><tr><td style="text-align:left">分布式训练</td><td style="text-align:left"><code>--distributed-world-size</code></td><td style="text-align:left">设置所有节点的GPU总数，等于可见GPU数量</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--distributed-num-procs</code></td><td style="text-align:left">fork的子进程数量，等于可见GPU数量</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--distributed-rank</code></td><td style="text-align:left">目前进程的级别</td><td style="text-align:left">默认：0 (未知)</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--distributed-backend</code></td><td style="text-align:left">分布式后端</td><td style="text-align:left">默认：nccl (未知)</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--distributed-init-method</code></td><td style="text-align:left">初始化方法</td><td style="text-align:left">(未知)</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--distributed-port</code></td><td style="text-align:left">端口号，如果设置了上一条就不用管</td><td style="text-align:left">默认：-1(未知)</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--device-id,--local-rank</code></td><td style="text-align:left">使用哪一个GPU</td><td style="text-align:left">默认：0(未知)</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--distributed-no-spawn</code></td><td style="text-align:left">即使有多个GPU也不生成多个进程</td><td style="text-align:left">默认：False(未知)</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--ddp-backend</code></td><td style="text-align:left">DistributedDataParallel后端</td><td style="text-align:left">默认：pytorch_ddp</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--fix-batches-to-gpus</code></td><td style="text-align:left">不要在gpu之间切换批次;这减少了整体的随机性，可能会影响精度，但避免了重新读取数据的成本</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--find-unused-parameters</code></td><td style="text-align:left">控制未使用参数检测</td><td style="text-align:left">默认：False （未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--gradient-as-bucket-view</code></td><td style="text-align:left"></td><td style="text-align:left">默认：False （未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--heartbeat-timeout</code></td><td style="text-align:left">如果N秒内进程没反应就Kill，N为-1的话不kill</td><td style="text-align:left">默认：-1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--broadcast-buffers</code></td><td style="text-align:left">在GPU之间复制不可训练的参数</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--slowmo-momentum</code></td><td style="text-align:left">slowMo动量设置</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--slowmo-base-algorithm</code></td><td style="text-align:left">基础算法sgd或localsgd</td><td style="text-align:left">默认:localsgd</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--localsgd-frequency</code></td><td style="text-align:left">localsgd的allreduce频率</td><td style="text-align:left">默认:3</td></tr><tr><td style="text-align:left">部分其他参数见文档</td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left"></td></tr><tr><td style="text-align:left">模型配置</td><td style="text-align:left"><code>--arch -a</code></td><td style="text-align:left">模型架构选择</td><td style="text-align:left">可选项项:transformer_tiny,transformer,transformer_iwslt_de_en, transformer_wmt_en_de,roberta_base…</td></tr><tr><td style="text-align:left">优化设置</td><td style="text-align:left"><code>--max-epoch</code></td><td style="text-align:left">到特定的epoch时停止训练</td><td style="text-align:left">默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--max-upadte</code></td><td style="text-align:left">到特定的update时停止训练</td><td style="text-align:left">默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--stop-time-hours</code></td><td style="text-align:left">到指定的小时数后停止训练</td><td style="text-align:left">默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--clip-norm</code></td><td style="text-align:left">梯度剪切</td><td style="text-align:left">默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--sentence-avg</code></td><td style="text-align:left">使用一批次的句子数量正则化梯度（而不是tokens数量）</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--update-freq</code></td><td style="text-align:left">在第i个epoch时按照每Ni个批次更新参数</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--lr</code></td><td style="text-align:left">在前N个epoch设置学习率，使用LR_N的话epochs&gt;N</td><td style="text-align:left">默认：0.25 （未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--stop-min-lr</code></td><td style="text-align:left">当学习率降到该值时停止训练，默认不停止</td><td style="text-align:left">默认：-1.0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--use-bmuf</code></td><td style="text-align:left">使用全局优化器来同步多GPU/shards上的模型</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--skip-remainder-batch</code></td><td style="text-align:left">如果设置这个选项，则会跳过最后一批不满的批次</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left">检查点</td><td style="text-align:left"><code>--save-dir</code></td><td style="text-align:left">保存检查点的路径</td><td style="text-align:left">默认：checkpoints</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--restore-file</code></td><td style="text-align:left">加载已保存检查点文件的路径</td><td style="text-align:left">默认：checkpoints</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--continue-once</code></td><td style="text-align:left">从该检查点继续，除非设置了<code>--restore-file</code></td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--finetune-from-model</code></td><td style="text-align:left">从预训练模型微调</td><td style="text-align:left">未知</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--reset-dataloader</code></td><td style="text-align:left">如果设置了，就不从checkpoints中加载dataloader state</td><td style="text-align:left">默认：False (未知)</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--reset-lr-sheduler</code></td><td style="text-align:left">如果设置了，就不从checkpoints中加载lr sheduler state</td><td style="text-align:left">默认：False (未知)</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--reset-meters</code></td><td style="text-align:left">如果设置了，就不从checkpoints中加载meters</td><td style="text-align:left">默认：False (未知)</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--reset-optimizer</code></td><td style="text-align:left">如果设置了，就不从checkpoints中加载optimizer state</td><td style="text-align:left">默认：False (未知)</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--save-interval</code></td><td style="text-align:left">每N个epoch保存checkpoint</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--save-interval-updates</code></td><td style="text-align:left">每N个epoch验证并保存checkpoint</td><td style="text-align:left">默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--keep-interval-updates</code></td><td style="text-align:left">保存上一个选项的最后N个checkpoints</td><td style="text-align:left">默认：-1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--keep-interval-updates-pattern</code></td><td style="text-align:left">和上一条同时使用，不删除第k个checkpoints，其中k%keep_interval_updates_pattern==0</td><td style="text-align:left">默认：-1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--keep-last-epochs</code></td><td style="text-align:left">保存最后N个epoch的checkpoints</td><td style="text-align:left">默认：-1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--keep-best-epochs</code></td><td style="text-align:left">保存N个最佳sorce的checkpoints</td><td style="text-align:left">默认：-1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--no-save</code></td><td style="text-align:left">不保存model和checkpoints</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--no-epoch-checkpoints</code></td><td style="text-align:left">只保存最后一个epoch和最佳的checkpoints</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--no-last-checkpoints</code></td><td style="text-align:left">不保存最后一个epoch checkpoints</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--no-save-optimizer-state</code></td><td style="text-align:left">不将optimizer state视为checkpoint的一部分</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--best-checkpoint-metric</code></td><td style="text-align:left">best checkpoint的标准</td><td style="text-align:left">默认：loss （<code>重要</code>）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--maximize-best-checkpoint-metric</code></td><td style="text-align:left">选择最大的metric value来保存最佳的checkpoints</td><td style="text-align:left">默认：False（未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--patience</code></td><td style="text-align:left">连续的N次验证性能没有提升则停止训练，会被<code>–validate-interval</code>影响</td><td style="text-align:left">默认：-1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--checkpoint-suffix</code></td><td style="text-align:left">检查点文件的后缀</td><td style="text-align:left">默认：空</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--checkpoint-shard-count</code></td><td style="text-align:left">如果checkpoints文件超过300GB,会将其切片防止存储不足。</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--load-checkpoint-on-all-dp-ranks</code></td><td style="text-align:left">在所有并行设备上加载checkpoints，从第一个</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--write-checkpoints-asynchronously, --save-async</code></td><td style="text-align:left">异步保存检查点，功能测试中</td><td style="text-align:left">默认：False</td></tr></tbody></table></div><h3 id="fairseq-generate参数"><a href="#fairseq-generate参数" class="headerlink" title="fairseq-generate参数"></a>fairseq-generate参数</h3><div class="table-container"><table><thead><tr><th style="text-align:left">命令</th><th style="text-align:left">参数</th><th style="text-align:left">用法</th><th style="text-align:left">可选项/备注</th></tr></thead><tbody><tr><td style="text-align:left">fairseq-generate</td><td style="text-align:left">使用已训练的模型翻译预先处理好的数据</td><td style="text-align:left"></td><td style="text-align:left"></td></tr><tr><td style="text-align:left">Generation</td><td style="text-align:left"><code>--path</code></td><td style="text-align:left">模型文件路径，冒号间隔</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--post-process，--remove-bpe</code></td><td style="text-align:left">去除BPE、字母分割等</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--quiet</code></td><td style="text-align:left">只输出最后的分数</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--model-overrides</code></td><td style="text-align:left">在生成时重载模型参数，替换训练时的参数</td><td style="text-align:left">默认：无</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--results-path</code></td><td style="text-align:left">保存评价结果的路径</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--beam</code></td><td style="text-align:left">beam size</td><td style="text-align:left">默认：5</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--nbest</code></td><td style="text-align:left">输出翻译的结果数</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--max-len-a</code></td><td style="text-align:left">生成句子最大长度ax+b,x是源句子长度</td><td style="text-align:left">同下使用，默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--max-len-b</code></td><td style="text-align:left">生成句子最大长度ax+b,x是源句子长度</td><td style="text-align:left">同上使用，默认：200</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--min-len</code></td><td style="text-align:left">最小的生成长度</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--match-source-len</code></td><td style="text-align:left">生成的句子长度和原句子长度相同</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--unnormalized</code></td><td style="text-align:left">compare unnormalized hypothesis scores</td><td style="text-align:left">默认：False（未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--no-beamable-mm</code></td><td style="text-align:left">在attention layers中不使用BeamableMM</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--lenpen</code></td><td style="text-align:left">长度惩罚，如果小于1.0倾向短句子，大于1.0则倾向长句子</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--unkpen</code></td><td style="text-align:left">未知单词惩罚，如果小于0产生更多unks，大于0产生更少</td><td style="text-align:left">默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--replace-unk</code></td><td style="text-align:left">替换unk，使用字典</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--sacrebleu</code></td><td style="text-align:left">使用sacrebleu</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--score-reference</code></td><td style="text-align:left">只给参考翻译打分</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--prefix-size</code></td><td style="text-align:left">给定前缀长度来初始化生成</td><td style="text-align:left">默认：0（未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--no-repeat-ngram-size</code></td><td style="text-align:left">ngram中不能重复生成</td><td style="text-align:left">默认：0（未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--sampling</code></td><td style="text-align:left">在假设中采样而不是使用beam search</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--sampling-topk</code></td><td style="text-align:left">从K个可能的下个字中采样而不是从全部词中采样</td><td style="text-align:left">默认：-1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--constraints</code></td><td style="text-align:left">词法约束解码过程</td><td style="text-align:left">可选：ordered,unordered（未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--temperature</code></td><td style="text-align:left">temperature for generation</td><td style="text-align:left">默认：1.0 （未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--diverse-beam-groups</code></td><td style="text-align:left">组数</td><td style="text-align:left">默认：-1（未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--diverse-beam-strength</code></td><td style="text-align:left">strength of diversity penalty for Diverse Beam Search</td><td style="text-align:left">默认：-1（未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--diverse-rate</code></td><td style="text-align:left">strength of diversity penalty for Diverse Siblings Search</td><td style="text-align:left">默认：-1（未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--print-step</code></td><td style="text-align:left">输出steps</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--decoding-format</code></td><td style="text-align:left">解码格式</td><td style="text-align:left">可选：unigram, ensemble, vote, dp, bs</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--no-seed-provided</code></td><td style="text-align:left">设置了，生成时就不初始化种子</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--eos-token</code></td><td style="text-align:left">设置停止的token</td></tr></tbody></table></div><p>generation省略：</p><ul><li>print-alignment</li><li>lm-path</li><li>lm-weight</li><li>iter-decode-eos-penalty</li><li>iter-decode-max-iter</li><li>iter-decode-force-max-iter</li><li>iter-decode-with-beam</li><li>iter-decode-with-external-reranker</li></ul><h2 id="fairseq可扩展部分"><a href="#fairseq可扩展部分" class="headerlink" title="fairseq可扩展部分"></a>fairseq可扩展部分</h2><h3 id="Tasks"><a href="#Tasks" class="headerlink" title="Tasks"></a>Tasks</h3><p>Tasks存储字典，并且对加载/迭代数据集提供帮助，初始化Model/Criterion，计算损失。用法示例如下：<br><img src="/img/fairseq_md/1.jpg" alt="用例"></p><div class="table-container"><table><thead><tr><th style="text-align:left">命令</th><th style="text-align:left">参数</th><th style="text-align:left">用法</th><th style="text-align:left">可选项/备注</th></tr></thead><tbody><tr><td style="text-align:left">Additional command-line arguments</td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left">待续未完</td></tr></tbody></table></div><h3 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h3><p>该部分定义网络前馈过程，封装可学习参数，可以设置模型架构<br><img src="/img/fairseq_md/3.jpg" alt="Additional command-line arguments"></p><h3 id="Criterions"><a href="#Criterions" class="headerlink" title="Criterions"></a>Criterions</h3><p>给定model和batch，计算损失函数</p><h3 id="Optimizers"><a href="#Optimizers" class="headerlink" title="Optimizers"></a>Optimizers</h3><p>根据梯度更新模型参数</p><h3 id="学习率管理"><a href="#学习率管理" class="headerlink" title="学习率管理"></a>学习率管理</h3><h3 id="数据加载和处理"><a href="#数据加载和处理" class="headerlink" title="数据加载和处理"></a>数据加载和处理</h3><h3 id="模型模块"><a href="#模型模块" class="headerlink" title="模型模块"></a>模型模块</h3><h2 id="fairseq-translation使用"><a href="#fairseq-translation使用" class="headerlink" title="fairseq-translation使用"></a>fairseq-translation使用</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Download and prepare the data</span></span><br><span class="line"><span class="built_in">cd</span> examples/translation/</span><br><span class="line">bash prepare-iwslt14.sh</span><br><span class="line"><span class="built_in">cd</span> ../..</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocess/binarize the data</span></span><br><span class="line">TEXT=examples/translation/iwslt14.tokenized.de-en</span><br><span class="line">fairseq-preprocess --source-lang de --target-lang en \</span><br><span class="line">    --trainpref <span class="variable">$TEXT</span>/train --validpref <span class="variable">$TEXT</span>/valid --testpref <span class="variable">$TEXT</span>/test \</span><br><span class="line">    --destdir data-bin/iwslt14.tokenized.de-en \</span><br><span class="line">    --workers 20</span><br></pre></td></tr></table></figure><p><img src="/img/fairseq_md/5.jpg" alt="命令行截图"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0 fairseq-train \</span><br><span class="line">    data-bin/iwslt14.tokenized.de-en \</span><br><span class="line">    --<span class="built_in">arch</span> transformer_iwslt_de_en --share-decoder-input-output-embed \</span><br><span class="line">    --optimizer adam --adam-betas <span class="string">&#x27;(0.9, 0.98)&#x27;</span> --clip-norm 0.0 \</span><br><span class="line">    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \</span><br><span class="line">    --dropout 0.3 --weight-decay 0.0001 \</span><br><span class="line">    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \</span><br><span class="line">    --max-tokens 4096 \</span><br><span class="line">    --eval-bleu \</span><br><span class="line">    --eval-bleu-args <span class="string">&#x27;&#123;&quot;beam&quot;: 5, &quot;max_len_a&quot;: 1.2, &quot;max_len_b&quot;: 10&#125;&#x27;</span> \</span><br><span class="line">    --eval-bleu-detok moses \</span><br><span class="line">    --eval-bleu-remove-bpe \</span><br><span class="line">    --eval-bleu-print-samples \</span><br><span class="line">    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric</span><br></pre></td></tr></table></figure><p><img src="/img/fairseq_md/6.jpg" alt="训练截图"><br><img src="/img/fairseq_md/7.jpg" alt="训练159个epoch截图"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fairseq-generate data-bin/iwslt14.tokenized.de-en \</span><br><span class="line">    --path checkpoints/checkpoint_best.pt \</span><br><span class="line">    --batch-size 128 --beam 5 --remove-bpe</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/img/fairseq_md/8.jpg" alt="在测试集上验证截图"></p>]]></content>
      
      
      
        <tags>
            
            <tag> fairseq </tag>
            
            <tag> 机器翻译 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
