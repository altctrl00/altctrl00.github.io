<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Multilingual Neural Machine Translation with Deep Encoder and Multiple Shallow Decoders</title>
      <link href="/2022/09/29/Multilingual_Neural_Machine_Translation_with_Deep_Encoder_and_Multiple_Shallow_Decoders/"/>
      <url>/2022/09/29/Multilingual_Neural_Machine_Translation_with_Deep_Encoder_and_Multiple_Shallow_Decoders/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>&emsp;&emsp;目前很多多语言翻译的模型使用的是<code>encoder-decoder</code>模型，如果加深两端的网络深度可以提高翻译的性能。举例子来说，在<code>many-to-one</code>的翻译设置中增加<code>encoder</code>的深度会提高翻译质量，可能的解释是<strong>encoder</strong>端的参数体量大了能记住更多的源语言特征。但是对<code>one-to-many</code>来说，增加<strong>decoder</strong>端的参数会使得推理变慢。因此在这里就会有一个网络深度和模型性能、推理速度的权衡设计。之前的论文<a href="https://arxiv.org/abs/2006.10369v1">Deep encoder, shallow decoder: Reevaluating the speed-quality tradeoff in machine translation.</a>提出了一种基于Transformer的模型——<strong>a deep encoder and a shallow decoder (DESD)</strong>。但这在<code>one-to-many</code>中会使得性能下降，可能是因为decoder端参数不足以为多个目标语言建模。因此为了能又快又准翻译，作者使用<strong>multiple shallow decoders</strong>，每个decoder对应于一类目标语言。其中作者探索了多种decoder-目标语言对应关系。</p><h3 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h3><ol><li>设置多种不同层数的<strong>DESD</strong>模型方案，在双语翻译和多语言翻译任务中权衡比较翻译速度(tokens/每秒)和准确性（BLEU）。</li><li>在<code>many-to-one</code>翻译中，<strong>多语言DESD</strong>模型以<strong>1.8x</strong>更快的平均推理加速比得到与基线模型同等的表现（模型参数量相当）</li><li>在<code>one-to-many</code>翻译中，提出了<strong>shared encoder and multiple shallow decoders(DEMSD)</strong>，得到<strong>1.8x</strong>更快的平均推理加速比，并且翻译质量更好。</li></ol><h2 id="多语言DESD模型的翻译速度和性能权衡"><a href="#多语言DESD模型的翻译速度和性能权衡" class="headerlink" title="多语言DESD模型的翻译速度和性能权衡"></a>多语言DESD模型的翻译速度和性能权衡</h2><h3 id="语料集"><a href="#语料集" class="headerlink" title="语料集"></a>语料集</h3><ol><li>ML50，49种语言&lt;-&gt;英语，两亿句子对。</li><li>TED8-Related，4种低资源语言，4种高资源语言。其中可以分为4个语言族，每个族2种语言</li><li>TED8-Diverse，8种语言，不考虑语言之间相关性。<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><img src="/img/Multilingual_Neural_Machine_Translation_with_Deep_Encoder_and_Multiple_Shallow_Decoders/1.jpg" alt="在三个数据集上的speed-quality trade-off,在三个数据集上训练过程相同"></li><li><strong>多于多对一翻译来说，减少了decoder端的层数其实对性能没有多大影响。这就说明其实对于decoder端，建模学习一种目标语言所需要参数量是很少的，也有可能是因为deep encoder更好地学习到了源语言，给出了更鲁棒的中间表示。</strong></li><li><strong>对于一对多翻译来说，当decoder端层数减少时性能会有明显下降。尤其是只有一两层时，在三个数据集上都是如此的趋势。这说明decoder端的参数量要和目标语言数量匹配（好奇1-11，2-10这样的层会是怎么样的效果）</strong></li><li>一对多翻译的结果其实都是远不如多对一，这可能和参数量无关，而是需要一种更有效的模型架构将一映射到多。（个人看法）</li></ol><h2 id="Deep-Encoder-and-Multiple-Shallow-Decoders（DEMSD）"><a href="#Deep-Encoder-and-Multiple-Shallow-Decoders（DEMSD）" class="headerlink" title="Deep Encoder and Multiple Shallow Decoders（DEMSD）"></a>Deep Encoder and Multiple Shallow Decoders（DEMSD）</h2><p>&emsp;&emsp;既要保持推理速度比较快又要性能比较高的话，前者一定要求decoder端层数少，后者可以减少decoder端所要表示的语言数量。<strong>有一种方法就是对于不同语言集对应一个shallow decoder</strong>。作者讨论了以下几种对应方法：</p><ol><li>最简单的放法就是每种语言一个decoder，<strong>(EACH)</strong></li><li>随机语言集合划分到一个decoder，每个decoder的语言集合大小相同，作者随机取三次求平均。<strong>(RAND)</strong></li><li>根据语言特征，每种语系一个decoder。<strong>（FAM）</strong></li><li>使用经典前置token的方法，将训练好的模型前置token抽取出来，每个token对应一种语言，根据token将语言分类(<strong>聚类？</strong>)，然后为每一类语言分配一个decoder。<strong>（EMB）</strong></li><li>上一种方法的缺点是需要预训练一个模型，然后再找这种对应关系，比较繁琐。希望寻找一种方法能够再训练过程中自动分配。<strong>(ST)</strong></li></ol><p>&emsp;&emsp;第5点的数学说明：在给定一个共享的encocder,$E$和N个decoders,$D=[D_1,D_2,…,D_N]$。对于一个源语言$L$，模型要先找到对应的一个decoder,$D_i$。再通过这个解码器得到解码预测的句子。整体的概率公式为$\log p(y|x,E,D_i)$,$y,x$分别是输出输入的句子。那么怎么找到对应的序号$i$呢？通过对语言$L$对应在模型中的具体向量$L_e$和decoder的条件概率建模。也即对于给定$L_e$，选择条件概率最大的decoder。公式为$i={\arg\max}_j\ p(j|L_e)$。希望模型能够学习到给定$L_e$，每个decoder的概率分布。但是因为argmax目前的形式是不可微的，也就无法求导更新参数。因此使用<code>Gumble-Softmax</code>作为argmax的一种可微形式。<br><img src="/img/Multilingual_Neural_Machine_Translation_with_Deep_Encoder_and_Multiple_Shallow_Decoders/2.jpg" alt="Gimble-Softmax具体形式，其中l是已经对数化了的，但是不知道具体是啥。g是加入的随机因素，t是线性减少的温度因子"><br>最后，在训练时要计算每个encoder的输出token概率分布加和。在推理时只需要选择$P(i|L_e)$最大的decoder。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>&emsp;&emsp;作者使用多个1层或2层的decoders，在三个数据集上进行了<code>one-to-many</code>实验。<br><img src="/img/Multilingual_Neural_Machine_Translation_with_Deep_Encoder_and_Multiple_Shallow_Decoders/3.jpg" alt="缩写词意思"><br><img src="/img/Multilingual_Neural_Machine_Translation_with_Deep_Encoder_and_Multiple_Shallow_Decoders/5.jpg" alt="实验结果1"><br><img src="/img/Multilingual_Neural_Machine_Translation_with_Deep_Encoder_and_Multiple_Shallow_Decoders/6.jpg" alt="实验结果2"></p><ol><li>对于<strong>(EACH)</strong>来说，这种方法在中高资源语言中获得更好的性能，但是在低资源语言中表现很差（与第二行对比）。在高资源语言中表现好是因为可能没有负面的迁移学习干扰，并且模型容量比较大，容易习得特征。在低资源中表现不好，还是表明一两层的decoder没办法学习到鲁棒的表示。</li><li>对于<strong>(RAND)</strong>来说，其实性能有小小的提升，这可能是因为一个语言集合中有相似的语言，迁移学习有用。增加少量语言数量性能还是会增加的，证明decoder端其实有个最多能接受多少语言数的阈值。</li><li>对于<strong>(FAM)</strong>来说，ML50,TED8-Related和TED8-Diverse分别划分了15,4,5个语言族。实验证明确实能够提升性能减少推理时间，尤其是对2层的decoder来说性能更好。相同语言之间的迁移学习对性能提升有很大帮助。</li><li>对于<strong>(EMB)</strong>来说，首先在将多种语言分类时就发现其实划分的结果和语言族划分的结果相同或相近，因此结果也和<strong>FAM</strong>相似。</li><li>对于<strong>(ST)</strong>来说，这种方法既不需要知道语言族，也不需要提前训练好一个模型。从BLEU上看，这种方法也能得到较好的结果，也是有效。</li></ol><h3 id="扩展分析"><a href="#扩展分析" class="headerlink" title="扩展分析"></a>扩展分析</h3><p><img src="/img/Multilingual_Neural_Machine_Translation_with_Deep_Encoder_and_Multiple_Shallow_Decoders/7.jpg" alt="在TED8-Related和TED8-Diverse数据集上的不同decoders层数实验"></p><ol><li>同样和之前一样有相同的趋势，decoder层数越少解码越快，但是性能越差。</li><li>使用10-2<strong>FAM、EMB</strong>方法在层数较少的情况下和6-6 baseline性能相似，但是解码快了近2倍。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&emsp;&emsp;之前的方法<strong>DSED</strong>在解码速度上有提升，但是在一对多翻译上性能很差，为了尽可能提高这种方法性能，作者提出了<strong>DEMSED</strong>，其中每一个decoder对应一类语言集，作者讨论了不同的对应方法，将相近的语言族对应一个decoder能够达到更快的1.8倍解码速度，其性能还能匹配标准的基线模型。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器翻译 </tag>
            
            <tag> 多语言机器翻译 </tag>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 论文阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Improving Zero-Shot Translation by Disentangling Positional Information</title>
      <link href="/2022/09/29/Improving_Zero-Shot_Translation_by_Disentangling_Positional_Information/"/>
      <url>/2022/09/29/Improving_Zero-Shot_Translation_by_Disentangling_Positional_Information/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>&emsp;&emsp;多语言翻译（NMT）在一个单独的模型中学习不同翻译方向的语句对，得到的模型能够直接在未知的翻译方向之间进行翻译。因此对于这种Zero-shot（在没有或很少训练语句对的翻译方向）翻译来说，是很有用的。首先不需要一个中介语言来辅助翻译，减少了一半推理时间和误差传播。其次不要求所有翻译方向具有大量的平行语句对语料，如果要实现N种语言之间互译的话不需要N^2数量级的方向上的语句对。最后对于模型来说，Zero-shot希望模型习得语句的中间表示，这两点对于低资源语言的翻译益处很大。<br>&emsp;&emsp;但是Zero-shot翻译的质量有待大幅提升，先前的工作发现其实标准的Transformer可能并没有真正习得不同语言对之间的翻译关联。在训练的时候学到的是将原句子以某种形式编码，这种形式便利了<strong>有监督情况下的目标语言的翻译</strong>，也就是<strong>编码成的中间表示其实关联着已知目标语言</strong>。但是对于Zero-shot翻译来说，很多翻译方向并不在训练中出现。decoder端的输入分布对于decoder来说是没有见过的，不可避免地造成性能下降。理想情况下，我们希望模型能够习得<strong>language-agnostic</strong>的中间表示，只要是在模型中训练过的target language,不管输入啥源语言都能根据这个中间表示翻译。<br>&emsp;&emsp;<strong>所以，我们似乎有了一个新的方向，就是统一不同语言版本encoder端输出的中间语言表示。</strong>这里来观察一个例子。<br><img src="/img/Improving_Zero-Shot_Translation_by_Disentangling_Positional_Information/1.jpg" alt="右图是西班牙语,逐词翻译是“一只猫大”"><br>&emsp;&emsp;观察encoder端的输出，<strong>输出向量紧紧关联于原句子对应位置的tokens（以下称为位置关联）</strong>。但是对于同一句话的不同语言版本经encoder输出，我们希望它是尽可能和语言无关的。因此需要encoder端对词语的排序有较大自由度，允许输出向量重排。<br>沿着这条路作者有三点主要贡献：</p><ol><li>实验证明位置关联确实会妨碍zero-shot翻译。移除了encoder layer中间一层的残差连接能够在zero-shot翻译上效果提升。</li><li>提出的模型易集成新语言，使得新语言和之前训练过的语言能够翻译，即使是zero-shot情况下。</li><li>深入分析了模型的中间输出，发现提出的方法有助于创造在token级别和句子级别上语言独立的表示。</li></ol><h2 id="消除位置关联信息"><a href="#消除位置关联信息" class="headerlink" title="消除位置关联信息"></a>消除位置关联信息</h2><p>&emsp;&emsp;虽然Transformer encoder的编码有利于上下文嵌入和序列标注，但是encoder端的输入和输出在位置上仍然具有很强的关联，<strong>没办法做到”any input language, same representation”</strong>。对于不同语言相同语义的句子来说，编码后的序列长度和词序是会变化的。针对位置关联来说，可能的两种原因是<strong>残差连接</strong>和<strong>编码器自注意力对齐</strong>（encoder self-attention alignment）。<br><img src="/img/Improving_Zero-Shot_Translation_by_Disentangling_Positional_Information/2.jpg" alt="消除位置关联的两种方法"></p><h3 id="修改残差连接"><a href="#修改残差连接" class="headerlink" title="修改残差连接"></a>修改残差连接</h3><p>&emsp;&emsp;在encoder layer中的两个子层都有残差连接，这种残差连接会导致输出和输入对应位置绑定。为了减轻这种影响，<strong>作者只在一层encoder layer中的一个multihead attention layer不使用残差连接</strong>。</p><h3 id="基于位置的自注意力查询"><a href="#基于位置的自注意力查询" class="headerlink" title="基于位置的自注意力查询"></a>基于位置的自注意力查询</h3><p>&emsp;&emsp;之前的实验表明在自注意力层的NxN注意力权重中对角线的权重都是较大的，因此输出中对应位置的输入会占较大部分，造成了输入输出在位置上相关联。从源头看，造成对角线权重较大的原因是<strong>Q,K来源于相同向量，虽然经过不同的投影，但是还是较为相似</strong>。为了减少Q,K相似性，使用一组正弦位置编码代替Q（<strong>sinusoidal positional encodings,为啥用这种操作？</strong>）,并且为了避免和K中就有的位置信息相似（注意encdoer的输入其实是加入位置信息的，这种信息会保留至K）。因此使用特定的波长（wave length），这个波长不同于起始编码加入的位置信息。此外，在得到真正Q的投影偏置中也使用了波长为100的位置编码（positional encoding with wave length 100.）。</p><h3 id="variational-dropout"><a href="#variational-dropout" class="headerlink" title="variational dropout"></a>variational dropout</h3><p>&emsp;&emsp;作者使用variational dropout，和标准的dropout的差别是每层的dropout不是随机的而是固定的。作者认为这种方法能够通过</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><p>&emsp;&emsp;训练数据包括高资源语言和低资源语言，训练翻译方向是<code>X&lt;-&gt;EN</code>，在测试时候是非英语的zero-shot翻译。模型为5层encoder,decoder的Transformer，对于较大的Europarl-full数据集使用8层。<br><img src="/img/Improving_Zero-Shot_Translation_by_Disentangling_Positional_Information/4.jpg" alt="使用的数据集信息"></p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/img/Improving_Zero-Shot_Translation_by_Disentangling_Positional_Information/3.jpg" alt="其中pivot前四行使用baseline模型，最后一个也是但是训练到收敛而不是早停，+Query意思是两种方法一起用"><br>几点结论：</p><ol><li>在zero-shot翻译上，修改残差连接是很有效的，在Supervised Directions也能和基线模型持平，但是Query的方法就不怎么有效。所以这种<strong>位置关联主要来源于残差连接</strong>。</li><li>观察第2，3行，得到的结论是<strong>训练数据越多样越好</strong>。第2行使用的是多平行句子作为数据集，第3行是没有重叠的数据。<strong>并且在zero-shot翻译上，修改残差连接更能有效利用更多样的数据。</strong></li><li>在zero-shot翻译上，Pivot方法也是非常有效的，和Residual的结果较为接近，但是在计算资源有限的情况下比较难以应用。</li><li>在句子数量较少，语言种类较多(<strong>尤其是lexical overlap较少</strong>)的数据集如PMIndia，修改残差连接的方法在zero-shot上的翻译很差，因此作者结合了variational dropout，发现效果有很大提升。<strong>但相较于Pivot方法来说还是差距很远，8左右。</strong><br><img src="/img/Improving_Zero-Shot_Translation_by_Disentangling_Positional_Information/5.jpg" alt="variational dropout在数据量较小或者文本重叠较少情况下的增益"><br>&emsp;&emsp;<strong>为了查看模型的扩展性，因此作者加入新的语言</strong>，具体的方法是使用之前的模型在包含“新语言-英语”的数据集上微调，并且在“新语言-非英语语言”进行测试。从结果可以看到，baseline其实过拟合了“新语言-英语”翻译方向，导致在zero-shot翻译方向上很差，作者提出的模型则有更好的泛化性，在原有翻译方向上也不错，在zero-shot上有较大的性能提升。<br><img src="/img/Improving_Zero-Shot_Translation_by_Disentangling_Positional_Information/6.jpg" alt="加入新的语言"><h3 id="实验分析"><a href="#实验分析" class="headerlink" title="实验分析"></a>实验分析</h3><h4 id="检验位置关联"><a href="#检验位置关联" class="headerlink" title="检验位置关联"></a>检验位置关联</h4>&emsp;&emsp;为了探索Residual方法是否真的减弱了位置关联性，作者训练一个分类器，将encoder输出向量映射至词汇表空间（<strong>Token ID</strong>）或者一句话timesteps空间（<strong>Position ID</strong>），发现Residual的方法确实会降低位置关联程度。再对encoder的每一层输出进行比较(<strong>更直观体现Residual方法</strong>)，发现在使用Residual的层相关性会立刻降低。<br><img src="/img/Improving_Zero-Shot_Translation_by_Disentangling_Positional_Information/7.jpg" alt="位置相关性的探索结果"><h4 id="检验语言独立性"><a href="#检验语言独立性" class="headerlink" title="检验语言独立性"></a>检验语言独立性</h4>&emsp;&emsp;为了探索Residual方法是否产生了语言无关的表示，作者使用<strong>SVCCV,Language Classification Accuracy</strong>两种方法，从token和sentence两种角度分析了不同语言在encoder端的输出是否相似，或者每一层的输出是否相似。（其中SVCCV将输出平均池化然后分析相似度，后者将输出分类到语言类别空间，分的越差说明越相似。）<h4 id="Zero-shot中直接翻译的优势"><a href="#Zero-shot中直接翻译的优势" class="headerlink" title="Zero-shot中直接翻译的优势"></a>Zero-shot中直接翻译的优势</h4>作者探讨了Zero-shot直接翻译相较于Pivot的优势，除了只需要一次翻译外，<strong>Zero-shot的翻译可能更地道，更符合语法。</strong><h4 id="使用固定的-可学习的位置嵌入"><a href="#使用固定的-可学习的位置嵌入" class="headerlink" title="使用固定的/可学习的位置嵌入"></a>使用固定的/可学习的位置嵌入</h4>作者分析了位置嵌入对最后结果的影响，发现作者提出的方法对位置嵌入的形式是鲁棒的。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&emsp;&emsp;作者认为encoder部分混杂了源语言词序的特点，这种位置关联特点影响了跨语言泛化能力，从而在zero-shot上性能差，作者提出了移除残差连接的方法减轻这种位置关联并在zero-shot上取得很好的性能。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器翻译 </tag>
            
            <tag> 多语言机器翻译 </tag>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 论文阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation</title>
      <link href="/2022/09/29/Improving_Massively_Multilingual_Neural_Machine_Translation_and%20Zero-Shot_Translation/"/>
      <url>/2022/09/29/Improving_Massively_Multilingual_Neural_Machine_Translation_and%20Zero-Shot_Translation/</url>
      
        <content type="html"><![CDATA[<h2 id="前人工作"><a href="#前人工作" class="headerlink" title="前人工作"></a>前人工作</h2><ul><li>之前的工作要么对于一对多翻译使用共享的encoder，对于多对多翻译使用特殊的attention机制，对于多对一翻译使用基于字符的输入，这些方法需要为每种语言精心设计encoder/decoder，限制模型扩展性。</li><li>后来有方法使用单一模型完成多语言翻译，但是这种方法将语言转化至同一空间，忽视语言特性。<ul><li>有方法尝试重组参数共享(reorganizing parameter sharing)</li><li>设计语言特异的参数生成器（designing language-specific parameter generators）</li><li>解耦多语言编码（decoupling multilingual word encodings）</li><li>语言聚类（language clustering）</li></ul></li><li>对于off-target的问题，有两种解决方法，一是引入跨语言正则化器（cross regularizers），例如alignment regularizer和consistency regularizer。二是通过回译产生平行语料（backtranslation），或者通过中间语言翻译（Pivot）</li></ul><h2 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h2><ol><li>以一种更加便捷的、端到端的language specific方法，以及深化NMT网络来缓解模型容量问题；</li><li>采用随机在线backtranslation(ROBT)来缓解zero-shot translation中的off-target问题。</li></ol><h2 id="具体提出的方法"><a href="#具体提出的方法" class="headerlink" title="具体提出的方法"></a>具体提出的方法</h2><ol><li>更深的网络模型，使用the depth-scaled initialization method去训练模型。</li><li>(<strong>LALN</strong>)对于每个目标语言，使用前置token作为正则化的偏置<img src="/img/Improving_Massively_Multilingual_Neural_Machine_Translation_and Zero-Shot_Translation/1.jpg" alt="language-aware layer normalization(LALN)"></li><li>(<strong>LALT</strong>)对于encoder端的输出，再接一个d*d的投影<img src="/img/Improving_Massively_Multilingual_Neural_Machine_Translation_and Zero-Shot_Translation/2.jpg" alt="Language-aware Linear Transformation(LALT)"></li><li>(<strong>ROBt</strong>)首先训练一个多语言模型，再对此模型微调，对于每个batch里的每个平行语句对（x1,y1）来说，随机选取语言集T,将y1翻译到T中，形成新的平行语句对（x2,y1）,(x3,y1)…..再去更新模型参数。<img src="/img/Improving_Massively_Multilingual_Neural_Machine_Translation_and Zero-Shot_Translation/3.jpg" alt="Random Online Backtranslation"></li></ol><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="训练设置"><a href="#训练设置" class="headerlink" title="训练设置"></a>训练设置</h3><p>&emsp;&emsp;使用作者提出的<code>OPUS-100</code>训练集。总共包含了100种语言。由于是english-centric的，一共包含了99种language pair，一共有大约55M的训练样本，对于每一种language pair，都有2000条验证、测试数据。<br>另外，在zero-shot translation设置中，总共包含Arabic、Chinese、Dutch、French、German、Russian6个语种，共15个语言对。</p><h4 id="One-to-Many-translation"><a href="#One-to-Many-translation" class="headerlink" title="One-to-Many translation"></a>One-to-Many translation</h4><p>&emsp;&emsp;99个EN-&gt;X语言对上训练，得到的结论:</p><ol><li>LALT/LALN是有效的，表明丰富语言知觉(language awareness)能够减轻模型容量不足的缺点。</li><li>越深的模型越好<br><img src="/img/Improving_Massively_Multilingual_Neural_Machine_Translation_and Zero-Shot_Translation/4.jpg" alt="One-to-Many translation"></li></ol><h4 id="Many-to-Many-translation"><a href="#Many-to-Many-translation" class="headerlink" title="Many-to-Many translation"></a>Many-to-Many translation</h4><p>&emsp;&emsp;使用english-centric的语料共99*2个方向进行训练，再分别在EN-&gt;X、X-&gt;EN上评估。得到的几点结论：</p><ol><li>在EN-&gt;X翻译上，多对多训练设置比一对多训练设置的表现来的差。可能的原因是模型容纳了更多的翻译方向，model capacity问题更明显。（table 2,3的Transformer行）</li><li>在X-&gt;EN翻译上，多对多训练设置比双语基线的表现来的好，说明多对多的训练设置，模型有较好的迁移能力。（table 4的1,2行）</li><li>在多对多翻译设置下。X-&gt;EN的提升要大于EN-&gt;X。这可能因为翻译方向上数据的分布不均，因为一半的数据都是-&gt;EN。</li><li>LALT/LALN更偏向于目标语言的区分，因此对于EN-&gt;X提升大于X-&gt;EN<br><img src="/img/Improving_Massively_Multilingual_Neural_Machine_Translation_and Zero-Shot_Translation/5.jpg" alt="Many-to-Many translation"></li></ol><h4 id="多对多翻译设置对不同资源语言的增益"><a href="#多对多翻译设置对不同资源语言的增益" class="headerlink" title="多对多翻译设置对不同资源语言的增益"></a>多对多翻译设置对不同资源语言的增益</h4><p>&emsp;&emsp;不同语言对的句子数量是不一样的，这会影响到language-aware建模和更深的Transformer翻译。因此作者根据训练数据大小将测试语言对划分为三种形式，High (≥ 0.9M, 45), Low (&lt; 0.1M, 18) and Medium (others, 31)，得到以下结论</p><ol><li>对于低资源语言来说，作者提出的方法在EN-&gt;X的翻译提升大于X-&gt;EN。</li><li>模型越深对于不同资源类型的语言和不同翻译方向上都更好。<br><img src="/img/Improving_Massively_Multilingual_Neural_Machine_Translation_and Zero-Shot_Translation/6.jpg" alt="不同规模训练语料"></li></ol><h4 id="在Zero-shot上的结果"><a href="#在Zero-shot上的结果" class="headerlink" title="在Zero-shot上的结果"></a>在Zero-shot上的结果</h4><p>&emsp;&emsp;之前的工作发现在多语言模型上进行zero-shot翻译也即未在训练集出现的翻译方向上表现很差，作者经过实验得到以下结论：</p><ol><li>模型容量不是表现差的主要原因，增大模型容量的性能提升也很少（第2,6行）</li><li>zero-shot上结果很差的主要原因是off-target问题。</li><li>使用ROBt方法在zero-shot上会有更大的提升，并且off-target问题得到改善。但同时在非zero-shot上也有性能微降（<strong>在其他论文中也发现了这个问题</strong>）<br><img src="/img/Improving_Massively_Multilingual_Neural_Machine_Translation_and Zero-Shot_Translation/7.jpg" alt="在Zero-shot上的结果"></li></ol><h4 id="ROBt的探究"><a href="#ROBt的探究" class="headerlink" title="ROBt的探究"></a>ROBt的探究</h4><ol><li>ROBt能够加快模型收敛，在几千步就完成。<img src="/img/Improving_Massively_Multilingual_Neural_Machine_Translation_and Zero-Shot_Translation/8.jpg" alt="ROBt收敛"></li><li>选择ROBt中的语言集合很关键，缩小集合范围能少量提升性能。<img src="/img/Improving_Massively_Multilingual_Neural_Machine_Translation_and Zero-Shot_Translation/9.jpg" alt="集合选择"></li></ol><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>&emsp;&emsp;作者为了解决多语言模型容量的问题使用了更深的模型架构和语言感知的模型（language-aware），并且提出random online backtranslation algorithm（ROBt）以解决zero-shot中的off-target问题。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器翻译 </tag>
            
            <tag> 多语言机器翻译 </tag>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 论文阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Language Tags Matter for Zero-Shot Neural Machine Translation</title>
      <link href="/2022/09/29/Language_Tags_Matter_for_Zero-Shot_Neural_Machine_Translation/"/>
      <url>/2022/09/29/Language_Tags_Matter_for_Zero-Shot_Neural_Machine_Translation/</url>
      
        <content type="html"><![CDATA[<h2 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h2><p>&emsp;&emsp;增强语义表示的一致性和减轻Off-target问题成为提升zero-shot翻译的两种有效方法，前者希望不同语言的语义表示尽可能接近，后者希望模型翻译的方向不要出错。作者调查了4种流行的LT(language target)，进行系统的分析。<br><img src="/img/Language_Tags_Matter_for_Zero-Shot_Neural_Machine_Translation/1.jpg" alt="四种LT"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>&emsp;&emsp;IWSLT17，TED talks，Europal v7.对于前两种数据集使用标准Transformer，最后一个数据集使用Transformer-big。学习联合的分词系统，并且一个batch近似有30k source tokens和target tokens.<br><img src="/img/Language_Tags_Matter_for_Zero-Shot_Neural_Machine_Translation/3.jpg" alt="数据集"></p><h3 id="实验结果分析"><a href="#实验结果分析" class="headerlink" title="实验结果分析"></a>实验结果分析</h3><p>&emsp;&emsp;T-ENC碾压了其他三种LT,BLEU很高并且off-target问题减少。</p><ol><li>加入TLT有助于zero-shot问题，但在encoder端收益最大。</li><li>加入SLT会干扰zero-shot翻译。</li></ol><p><img src="/img/Language_Tags_Matter_for_Zero-Shot_Neural_Machine_Translation/3.jpg" alt="实验结果"></p><h3 id="可视化分析"><a href="#可视化分析" class="headerlink" title="可视化分析"></a>可视化分析</h3><ol><li><p>T-ENC有助于加强语义表示的一致性，在KDE上重叠程度越高，说明表示越一致。<img src="/img/Language_Tags_Matter_for_Zero-Shot_Neural_Machine_Translation/4.jpg" alt="KDE"></p></li><li><p>T-ENC能有效减轻off-target问题，可以从对LT的注意力系数得出，T-ENC在解码时受到了更多的关注，S-ENC-T-ENC也是，但是S-ENC会干扰，使得模型困惑。<img src="/img/Language_Tags_Matter_for_Zero-Shot_Neural_Machine_Translation/5.jpg" alt="Attention"></p></li><li><p>关注many-to-one和one-to-many设置下encoder端和decoder端输出的相似性。</p><ul><li>在多对一中，不同语言的语义一致性随着层数增加而提升，但在encoder和decoder交界处有突降，这可能是decoder干扰了encoder。其中T-ENC下降最不明显，因此一致性保留得越好。</li><li>在一对多中，语义相似性随着层数增多不断下降，其中T-ENC下降最大，更好地引导模型生成相应语言。<br><img src="/img/Language_Tags_Matter_for_Zero-Shot_Neural_Machine_Translation/6.jpg" alt="layer-wise similarity"></li></ul></li></ol><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ol><li>不要用SLT,会干扰TLT的作用。</li><li>使用TLT-ENC更有助于生成目标语言和更相近的中间表示。</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 机器翻译 </tag>
            
            <tag> 多语言机器翻译 </tag>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 论文阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Alternative Input Signals Ease Transfer in Multilingual Machine Translation</title>
      <link href="/2022/09/19/Alternative_Input_Signals_Ease_Transfer_in_Multilingual_Machine_Translation/"/>
      <url>/2022/09/19/Alternative_Input_Signals_Ease_Transfer_in_Multilingual_Machine_Translation/</url>
      
        <content type="html"><![CDATA[<h1 id="Alternative-Input-Signals-Ease-Transfer-in-Multilingual-Machine-Translation"><a href="#Alternative-Input-Signals-Ease-Transfer-in-Multilingual-Machine-Translation" class="headerlink" title="Alternative Input Signals Ease Transfer in Multilingual Machine Translation"></a>Alternative Input Signals Ease Transfer in Multilingual Machine Translation</h1><h2 id="前情提要"><a href="#前情提要" class="headerlink" title="前情提要"></a>前情提要</h2><p>MMT对低资源语言来说很有用，因为在模型可以利用不同语言的相似性和不同语言之间的共享信息来提高低资源语言的翻译效果。对于不同语言的共享信息这一点来说，最基本的就是两个语言之间的重叠tokens,这些tokens通常含有相同的含义，低资源语言的翻译能借鉴模型中存储的高资源语言的信息。如果语言间的重叠很少：例如只有标点符号和数字，那么就没办法有效利用这部分信息。此外，如果两种语言处于不同书写的字体系统时，重叠也会很少。</p><h2 id="语言共享信息方法"><a href="#语言共享信息方法" class="headerlink" title="语言共享信息方法"></a>语言共享信息方法</h2><p>为了解决由于不同书写系统（<code>scripts</code>、<code>writting systems</code>）引起的不同语言之间的重叠太少问题，作者提出了以下三种信号转化方法（<code>transliteration</code>、<code>signal system</code>）</p><h3 id="国际音标法"><a href="#国际音标法" class="headerlink" title="国际音标法"></a>国际音标法</h3><p>很多语言中的许多词发音是相同的，但是在不同的书写系统来看他们是完全不重叠的。因此为了运用这部分重叠的信息，将文本通过国际音标转为音标文本，其中音标的单位是音素。当然这种方法也会引入噪声，例如：</p><ol><li>标点符号可能在转化过程中丢失（或许可以标记增加断句信息？）</li><li>对于一种语言来说不同词可能会发相同的音，但是是完全的不同字形。</li><li>一个字可能还有不同发音，不同发音的意思不同（<strong>个人想法</strong>）</li></ol><h3 id="罗马化法"><a href="#罗马化法" class="headerlink" title="罗马化法"></a>罗马化法</h3><p>在现代化过程中，很多语言都能通过键盘上的26个字母输入到电脑中，因此语言或多或少具有这种罗马形式（例如汉字拼音）。</p><h3 id="转化为其他语言字体法"><a href="#转化为其他语言字体法" class="headerlink" title="转化为其他语言字体法"></a>转化为其他语言字体法</h3><p>之前两种方式会增加模型需要学习的语言表示量，并且词汇表也要相应扩充。这种方法是将一种语言转化为同源语言族中一种语言，这样也不需要提前学习subword等。</p><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><p>利用这三种方法不需要额外的模型，例如第一种方法可以直接使用音标对齐直接转化（可以利用工具包<code>espkea-ng</code>），第二种方法可以使用工具包<code>indic-trans</code>，第三种可以用字母表对齐实现(手动实现)。</p><h2 id="多种转化后的信号融合"><a href="#多种转化后的信号融合" class="headerlink" title="多种转化后的信号融合"></a>多种转化后的信号融合</h2><h3 id="Straight-Concatenation"><a href="#Straight-Concatenation" class="headerlink" title="Straight Concatenation"></a>Straight Concatenation</h3><p>简单，不需要改变模型架构，使用特殊的tokens分隔不同输入，代价是句子长的话就需要更多的计算。</p><h3 id="Multi-Encoder-Architectures"><a href="#Multi-Encoder-Architectures" class="headerlink" title="Multi-Encoder Architectures"></a>Multi-Encoder Architectures</h3><p>为每个信号准备一个Encoder，也就是多个Encoder,一个decoder模型，多个encoder组合方式不同，主要有4种（<a href="https://arxiv.org/abs/1811.04716v1">Libovick ́ y et al. (2018)</a>），对应的encoder-decoder attention（cross attention）也有不同形式。<strong>虽然之前证明有效，但是还是需要更巧妙的模型设计才能达到更好的性能</strong></p><h3 id="Multi-Source-Ensemble"><a href="#Multi-Source-Ensemble" class="headerlink" title="Multi-Source Ensemble"></a>Multi-Source Ensemble</h3><p>多个模型，不同模型除了输入信号不同其他都相同，最后将每个模型输出的概率分布平均。缺点：超多计算。</p><h3 id="Multi-Source-Self-Ensemble"><a href="#Multi-Source-Self-Ensemble" class="headerlink" title="Multi-Source Self-Ensemble"></a>Multi-Source Self-Ensemble</h3><p>借鉴了集成多种信号的优点，但是只需要训练一个模型。对于一个句子来说，转为不同信号，每个信号前面加上特定的token，重复经过模型得到概率分布，最后将概率分布平均。<br><img src="/img/Alternative_Input_Signals_Ease_Transfer_in_Multilingual_Machine_Translation/1.jpg" alt="模型结构图"></p><h2 id="实验分析"><a href="#实验分析" class="headerlink" title="实验分析"></a>实验分析</h2><p>翻译方向为<code>XX-&gt;EN</code>，语言族：<code>Indic,Turkic</code><br><img src="/img/Alternative_Input_Signals_Ease_Transfer_in_Multilingual_Machine_Translation/2.jpg" alt="实验结果"><br>对于<strong>多源自集成</strong>来说：</p><ol><li>单独的转化后的信号不会使得BLEU提升，甚至有些下降。原因是引入的干扰大于有效信息。</li><li>单独的转化后的信号和原始信号一起输入会使得BLEU提升，但是最多在1.0左右</li><li>将encoder参数量提升一倍，BLEU的值提升1.3。</li></ol><h3 id="多源自集成优势"><a href="#多源自集成优势" class="headerlink" title="多源自集成优势"></a>多源自集成优势</h3><ol><li>seq2seq模型架构不需要做改变</li><li>在低资源语料的情况下，比基线水平的BLEU提升更大达到5.0。并且随着数据集大小增大始终优于基线水平。</li><li>生成的假设较为一致，作者提出的C-BLEU更好（We treat the output of L1-En direction as reference and out-put of all other Li-En directions as hypothesis. We compute this for all N source languages in the dataset, accounting for total N (N − 1) C-BLEU scores, then take the average of all）</li><li>生成的句子使用NER去抽取实体，不同类别的实体识别的效果都优于基线。表明生成句子更准确。</li></ol><p><img src="/img/Alternative_Input_Signals_Ease_Transfer_in_Multilingual_Machine_Translation/3.jpg" alt="第二点"><br><img src="/img/Alternative_Input_Signals_Ease_Transfer_in_Multilingual_Machine_Translation/4.jpg" alt="第三点"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者为了解决不同语言之间信息重叠过少的问题，提出了三种不同的信号转化方式和多源自集成方法使得翻译性能提升，准确性也有提升。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器翻译 </tag>
            
            <tag> 多语言机器翻译 </tag>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 论文阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?</title>
      <link href="/2022/09/19/Pre-Trained_Multilingual_Sequence-to-Sequence_Models_A_Hope_for_Low-Resource_Language_Translation/"/>
      <url>/2022/09/19/Pre-Trained_Multilingual_Sequence-to-Sequence_Models_A_Hope_for_Low-Resource_Language_Translation/</url>
      
        <content type="html"><![CDATA[<h1 id="Pre-Trained-Multilingual-Sequence-to-Sequence-Models-A-Hope-for-Low-Resource-Language-Translation"><a href="#Pre-Trained-Multilingual-Sequence-to-Sequence-Models-A-Hope-for-Low-Resource-Language-Translation" class="headerlink" title="Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?"></a>Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>该篇论文主要以mBART(一种预训练端到端多语言模型)为模型架构探索数据集对性能的影响，也形成了一种衡量模型的数据敏感性的框架，经过实验，文章提出的一个结论是<strong>与其想着怎么微调模型，还不如从数据上改进以更大提升表现，增强实用性。</strong></p><p>PMSS(预训练端到端多语言模型)<strong>数据集敏感性</strong>可探究的几个方面：</p><ul><li>微调训练时所需的数据量</li><li>微调数据集中的噪声</li><li>预训练数据集数量</li><li>领域不匹配问题</li><li>语言类型问题</li></ul><h3 id="mBART模型介绍"><a href="#mBART模型介绍" class="headerlink" title="mBART模型介绍"></a>mBART模型介绍</h3><p>首先这是一个纯用Transformer架构的预训练模型，预训练数据是Common Crawl,具有多种语言的文本数据。mBART的预训练目标是单语形式的，将原句子一定程度上破坏之后使用模型重建这个句子。这种目标函数比较有意思，它不引导模型产生相似的tokens或者representations。（<strong>疑问点，待续</strong>）在经过预训练后可以有监督或者直接进行翻译训练、预测。</p><h2 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h2><h3 id="评测语言选择"><a href="#评测语言选择" class="headerlink" title="评测语言选择"></a>评测语言选择</h3><p>文章选了10种语言，8种低资源语言和2种高资源语言（FR,HI）。在字体方面也有分类，主要为Latin和非Latin。在未知语言上也有分类，有4种语言是mBART在预训练未接触过的。具体统计如下图：<br><img src="/img/Pre-Trained_Multilingual_Sequence-to-Sequence_Models_A_Hope_for_Low-Resource_Language_Translation/2.jpg" alt="评测语言的具体信息,Joshi class用来划分资源高低，大于3算高资源语言"></p><h3 id="平行语料"><a href="#平行语料" class="headerlink" title="平行语料"></a>平行语料</h3><p>语料选择上既有Common Crawl（数量大、开放域、使用自动对齐上可能有噪声）、Bible等。包括开放域和领域特定的语料、并且将训练数据按照数量大小进行切割以进行下一步实验。<br><img src="/img/Pre-Trained_Multilingual_Sequence-to-Sequence_Models_A_Hope_for_Low-Resource_Language_Translation/3.jpg" alt="语料具体信息"></p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>实验以EN-&gt;XX和XX-&gt;EN为翻译方向。每个语言有3个测试集（2个特定域，FLORES是开放域），分别使用不同数量、不同类型的数据集进行微调训练得到结果。</p><p><img src="/img/Pre-Trained_Multilingual_Sequence-to-Sequence_Models_A_Hope_for_Low-Resource_Language_Translation/4.jpg" alt="实验结果1"></p><p><img src="/img/Pre-Trained_Multilingual_Sequence-to-Sequence_Models_A_Hope_for_Low-Resource_Language_Translation/5.jpg" alt="实验结果2"></p><h3 id="实验分析"><a href="#实验分析" class="headerlink" title="实验分析"></a>实验分析</h3><h4 id="微调训练时所需的数据量"><a href="#微调训练时所需的数据量" class="headerlink" title="微调训练时所需的数据量"></a>微调训练时所需的数据量</h4><ol><li>使用Common Crawl（开放域）,mBART模型使用25k的数据在性能上远超用100k数据的transformer模型，这在大多数语言上都得到验证，也有一两个低资源语言两者的表现都很不好。（test时候语言是预训练时就接触到的模型）</li><li>使用特定域的数据进行微调，结果也是mBART优于transformer。</li><li>微调训练的句子对数将在<code>50k</code>左右达到一个饱和，数量加大的话会导致预训练的参数改变过多，预训练保留的信息被冲掉，（mBART是标准Transformer架构，参数量约<code>680M</code>，预训练语料远大于50k）</li></ol><h4 id="微调数据集中的噪声"><a href="#微调数据集中的噪声" class="headerlink" title="微调数据集中的噪声"></a>微调数据集中的噪声</h4><p>这里噪声指的是是否和领域相关的数据。根据实验结果表明，mBART微调时相同领域的训练数据只需要开放域训练数据量的十分之一就能达到更好的效果。</p><h4 id="预训练数据集数量"><a href="#预训练数据集数量" class="headerlink" title="预训练数据集数量"></a>预训练数据集数量</h4><p><strong>mBART模型比transfomer进步的地方在于性能提升上，高资源语言更显著</strong>，个人认为句法结构和词语的多样性对于数据集来说很重要，因为模型的可拓展能力可能不够，没办法像人一样举一反三。那对应语料的数据量越大涵盖的词语和句子更多变，使得模型能够更好的将语言特征通过预训练保存到参数中，反过来说mBART确实能够很好利用预训练学习到，因为观察出了不同语言的差异。<br><img src="/img/Pre-Trained_Multilingual_Sequence-to-Sequence_Models_A_Hope_for_Low-Resource_Language_Translation/6.jpg" alt="预训练语料中语言的数量和实验的BLEU值关系"></p><h4 id="领域不匹配问题"><a href="#领域不匹配问题" class="headerlink" title="领域不匹配问题"></a>领域不匹配问题</h4><ol><li>领域相同的训练集和测试集效果要比不匹配的情况好。</li><li>开放域训练的模型可能在特定领域的测试集上表现得比开放域数据集好。</li><li>预训练数据越大越能在特定域数据集上表现出色，可以弥补特定域数据集不足的问题。</li></ol><h4 id="语言类型问题"><a href="#语言类型问题" class="headerlink" title="语言类型问题"></a>语言类型问题</h4><ol><li>如果一个低资源语言和目标语言有较多的token片段重叠或者相似的句法特征（主谓宾顺序、辅音突变等语法现象），那么在翻译的时候就可以得到更好的结果。</li><li><strong>翻译成英语总比英语翻译成其他语言的BLEU要好</strong>。这可以说是decoder更好地学到了英语这个语言，也有可能是由于BLEU不将子词纳入计算范围，这导致有些词素是对的但是整个词错了，从而导致翻译成其他语言的BLEU值不高。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>对于开放域的翻译来说，mBART比原始transformers架构只需要更少的微调数据就能达到更好的效果（<code>4-10倍</code>）</li><li>对于特定域的翻译来说，mBART数据量有效性为<code>5-10倍</code>,并且同时也更鲁棒，在领域外表现更好。</li><li>对于预训练数据集中没有出现的语言来说，BLEU表现非常差劲基本用不了。</li></ol><p>对于标题给出的疑问，PMSS能不能改善低语言资源，作者的回答是<strong>与其想着怎么微调模型，还不如想想怎么为低资源数据收集更多更好的训练句子</strong>（寄，不知道后面有无后续PMSS的文章）</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器翻译 </tag>
            
            <tag> 多语言机器翻译 </tag>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 论文阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Breaking Down Multilingual Machine Translation</title>
      <link href="/2022/09/19/Breaking_Down_Multilingual_Machine_Translation/"/>
      <url>/2022/09/19/Breaking_Down_Multilingual_Machine_Translation/</url>
      
        <content type="html"><![CDATA[<h1 id="Breaking-Down-Multilingual-Machine-Translation"><a href="#Breaking-Down-Multilingual-Machine-Translation" class="headerlink" title="Breaking Down Multilingual Machine Translation"></a>Breaking Down Multilingual Machine Translation</h1><h2 id="前情摘要"><a href="#前情摘要" class="headerlink" title="前情摘要"></a>前情摘要</h2><p>（之前文章提到过，多对一的翻译方向设置比一对多的翻译对双语翻译的性能提高更高。）<br>此外之前还有很多分析多语言翻译的文章如（<a href="https://arxiv.org/abs/1909.02197">Kudugunta et al., 2019</a>,<a href="https://arxiv.org/abs/1905.09418v1">Voita et al., 2019a</a>,<a href="https://aclanthology.org/2020.acl-main.688/">Aji et al., 2020</a>,<a href="https://aclanthology.org/2020.lrec-1.458">Mueller et al., 2020</a>）对多语言翻译模型进行了一些分析，但是这些分析并没有去探究<strong>不同翻译方向（如</strong><code>many-to-one</code><strong>）对模型中不同组成的影响，也没有检查模型中不同组成（</strong><code>encoder/decoder</code><strong>）各自的影响</strong>。这就是这篇文章主要探索的两部分。</p><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><p>数据集：TED Talks Dataset。将全部文本进行BPE分词，包括全部语言，词汇表<code>32000</code>。模型架构是Transformer。</p><h2 id="多语言训练如何影响模型中各个部分"><a href="#多语言训练如何影响模型中各个部分" class="headerlink" title="多语言训练如何影响模型中各个部分"></a>多语言训练如何影响模型中各个部分</h2><p><strong>之前的实验表明，多语训练的模型比双语模型性能来的更好</strong>，为了探究到底是对encoder端还是decoder端有效，作者进行了实验。<br><img src="/img/Breaking_Down_Multilingual_Machine_Translation/1.jpg" alt="实验结果1"><br><img src="/img/Breaking_Down_Multilingual_Machine_Translation/2.jpg" alt="实验结果2"></p><p>其中<code>Bilingual Only</code>是只使用双语语料并且从零开始训练的模型。<code>Load xxx</code>表示从训练好的多语言模型加载对应部分参数，并且这部分参数是可以训练的。<code>Freeze xxx</code>同样是加载对应参数但是冻结的，不可更新的。之所以提出freeze是因为可能模型其他部分的随机初始化会使得这部分的效果得到削弱。从左到右前4个是低资源语言，后4个是高资源语言。</p><h3 id="实验结果分析"><a href="#实验结果分析" class="headerlink" title="实验结果分析"></a>实验结果分析</h3><p><strong>对低资源语言来说:</strong></p><ol><li>多语言训练通常对encoders和decoders都是有益的。<code>load Enc.</code>和<code>freeze Dec.</code>再次证实encoders和decoders确实会保留多语言训练中的有效信息，比只用双语语料训练来的有效。</li><li>多语言训练对encoders来说更有效，load encoder和freeze encoder的表现优于load decoder和load decoder。’</li></ol><p><strong>对高资源语言来说:</strong></p><ol><li>多语言训练通常对encoders是有益的，但在部分情况下对decoders不是有益的，<code>load Enc.</code>性能提升明显，但是在<code>load Enc.</code>和<code>freeze Dec.</code>表现不比基线好。</li><li>多语言训练对encoder来的更重要，<code>load Enc.</code>能得到更好的结果。</li></ol><p><strong>对两种类型语言来说</strong></p><ol><li>load both普遍要比只load其中一种好。</li><li>对于<code>en-&gt;xx</code>方向，load encoders会优于freeze encoders。对于<code>xx-&gt;en</code>方向，freeze decoders要优于load decoders（只有ALL-En中it-&gt;en不是，在低资源语言翻译上更明显）。前者证明了其实encoder端是和target language有关的。（因为load的话参数是会改变的，更有效地根据具体的语言将target language的信息加入到encoder中）,后者<strong>有待思考</strong></li></ol><h2 id="模型参数是如何共享于不同语言对"><a href="#模型参数是如何共享于不同语言对" class="headerlink" title="模型参数是如何共享于不同语言对"></a>模型参数是如何共享于不同语言对</h2><p><strong>Head importance estimation</strong>（<a href="https://arxiv.org/abs/1905.10650v1">Michel et al. (2019)</a>）<br>该方法主要利用的是损失对应于每个头的梯度来衡量每个头的重要性。在每个attention中对各个头的梯度进行标准化后得到每个头的重要性得分，具体定义如下图：<br><img src="/img/Breaking_Down_Multilingual_Machine_Translation/3.jpg" alt="Head importance estimation定义"><br><img src="/img/Breaking_Down_Multilingual_Machine_Translation/4.jpg" alt="Head importance estimation定义"></p><p><strong>对于一个Head来说，如果在两个语言对上的得分都很高的话，那么这个Head就是重要的并且是共享的。</strong></p><h3 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h3><p>实验方法：一组语言对的所有训练句子可以得到相应的<strong>一组</strong>Head-importance得分，将其转为等级排序，对不同语言对来说就可以利用这<strong>两组</strong>数据计算斯皮尔曼等级相关系数。那么这个相关系数有什么用呢，怎么用来衡量模型中不同组成部分的共享情况呢？ 答案就是：<strong>相关系数越高——&gt;一致性越高——&gt;参数共享程度越大</strong>。举例子说人话：<strong>比如有一部分参数P，语言对a在这上面的梯度（或importance score或者importance rank）和语言对b高度相似并且都很大的话，那么可以推测两个语言对在参数P上对结果（loss）的影响是高度相似的，反过来说也就是是参数P在不同语言对中是共享的。</strong>据此作者比较了encoder和decoder端的参数共享程度。<br><img src="/img/Breaking_Down_Multilingual_Machine_Translation/5.jpg" alt="实验结果"></p><p>根据这个结果可以分析不同语言对之间的信息是如何在不同部分共享/存在的。</p><h4 id="Encoder-for-En-X"><a href="#Encoder-for-En-X" class="headerlink" title="Encoder for En-X"></a>Encoder for En-X</h4><p>对于encoders端来说，EN-X的语言对得到的相关系数（0.806和0.813）都要小于X-EN得到的相关系数（0.871和0.898）。这可能表明encoder端的部分参数是用来编码生成和目标语言有关的中间表示。</p><h4 id="Encoder-for-X-En"><a href="#Encoder-for-X-En" class="headerlink" title="Encoder for X-En"></a>Encoder for X-En</h4><p>X-EN得到的相关系数（0.871和0.898）都很高，说明encoder端是有可能高度参数共享的。</p><h4 id="Decoder-for-En-X"><a href="#Decoder-for-En-X" class="headerlink" title="Decoder for En-X"></a>Decoder for En-X</h4><p>虽然系数很低，但是在部分语言对中是共享的。对于一个模型来说尽可能是要在多语言中都有效。</p><h4 id="Decoder-for-X-En"><a href="#Decoder-for-X-En" class="headerlink" title="Decoder for X-En"></a>Decoder for X-En</h4><p>系数都很高，这表明decoder端参数是高度共享的。虽然之前文章有人证明encoder生成的中间表示是目标语言无关的，但是decoder端的重要参数取决于目标语言（也即需要这部分参数去学习目标语言之间不同之处），这就解释了多语言训练为什么不有利于<code>XX-EN</code>中decoder。其中的一个启发点就是如果要使得<code>XX-EN</code>翻译更好的话，多语言训练应该暴露更多的<code>XX-EN</code>语料，其中英文句子要更多样。</p><h2 id="依据参数共享改进"><a href="#依据参数共享改进" class="headerlink" title="依据参数共享改进"></a>依据参数共享改进</h2><p>对于多语言训练来说选择什么样的语言去训练会导致最后的双语翻译结果不一样，直觉上我们认为如果用相近的语言去预训练，最后得到的结果会更好。但有时候不是这样，用完全不相关的语言去训练可以达到更好的效果。因此怎么选择训练的语言成为一个重要问题，作者提出一种衡量标准，也就是要选择<strong>能够使得参数更好共享的语言</strong>。</p><h3 id="Improving-X-En-by-Related-En-X-Pairs"><a href="#Improving-X-En-by-Related-En-X-Pairs" class="headerlink" title="Improving X-En by Related En-X Pairs"></a>Improving X-En by Related En-X Pairs</h3><p>改进的思路，之前的实验说明对语言训练可能对decoder端的增益没有比encoder端大，这可能说明decoder端的参数共享程度比较低。所以我们可以通过选择使得decoder端的共享程度高的语言集合，具体计算如下：<br><img src="/img/Breaking_Down_Multilingual_Machine_Translation/6.jpg" alt="语言集合选择计算"></p><p><img src="/img/Breaking_Down_Multilingual_Machine_Translation/7.jpg" alt="具体结果"><br>可以看到使用ALL-En语料微调的效果最好，其次是使用ALL-En语料+EN-ALL语料（其中ALL是计算得到的<strong>相关语言</strong>）。这也说明<strong>ALL-EN和EN-ALL训练语料如何使用会是一个难题，有待进一步探究和运用</strong>。</p><h3 id="Improving-En-X-by-Language-Clusters"><a href="#Improving-En-X-by-Language-Clusters" class="headerlink" title="Improving En-X by Language Clusters"></a>Improving En-X by Language Clusters</h3><p>和上一种方法思路类似，要用相关的语言对改进EN-X的表现，逻辑：<strong>使用相关语言对——&gt;能够使得参数共享越容易——&gt;参数共享程度越高——&gt;性能表现更好</strong>。<br>为了分析方便，作者使用t-SNE对一组head-importance scores进行降维，作者只关注decoders端的得分，因为decoders端的EN-ALL语料上的相关系数很低。降维完后，如果两种语言接近的话，那么作者认为其是相关的。<br><img src="/img/Breaking_Down_Multilingual_Machine_Translation/8.jpg" alt="t-SNE处理后得到语言分布"></p><p><img src="/img/Breaking_Down_Multilingual_Machine_Translation/9.jpg" alt="实验结果"><br>得到相近的语言族后作者进行了实验，得到以下结论</p><ol><li>对于EN-ALL、ALL-ALL模型来说，在相近语言族上微调能够提升性能。</li><li>对于低资源语言来说总体在相近语言族上微调要优于在随机语言组上微调。</li><li>虽然图表中看起来使用相近的语言族并没有提升很多，这可能是因为相关系数阈值不够大，如果阈值选为0.8得到最后一行的结果，在两种方向语言翻译上得到较大提升。</li></ol><p>为了验证这种使用语言族的方法会不会真的使得decoders端参数共享程度增加，作者进行了实验。<br><img src="/img/Breaking_Down_Multilingual_Machine_Translation/10.jpg" alt="使用语言族微调后的decoders端参数共享情况"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器翻译 </tag>
            
            <tag> 多语言机器翻译 </tag>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 论文阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>wandb在fairseq的使用</title>
      <link href="/2022/09/04/wandb/"/>
      <url>/2022/09/04/wandb/</url>
      
        <content type="html"><![CDATA[<h2 id="注册wandb账号"><a href="#注册wandb账号" class="headerlink" title="注册wandb账号"></a>注册wandb账号</h2><p>首先到<a href="https://wandb.ai/site">wandb官网</a>注册一个账号，可以用github账号注册。<br><img src="/img/wandb/1.jpg" alt="个人账户页面">,可以提前创建个project，或者之后使用fairseq命令行创建新project。</p><h2 id="在命令行中创建wandb项目"><a href="#在命令行中创建wandb项目" class="headerlink" title="在命令行中创建wandb项目"></a>在命令行中创建wandb项目</h2><p>在之前创建账号时会自动产生一个API key，需要记住这个API key，如果没找到也可以在个人设置中找到。<br><img src="/img/wandb/3.jpg" alt="API key"><br>和之前一篇fairseq文章相同，在训练时多增加一个命令行参数<code>--wandb-project t</code>，其中t是wandb project名，如果不存在自动新建。<br>第一次启动wandb会要求输入API key,将其复制输入即可。<br><img src="/img/wandb/2.jpg" alt="输入API key"><br>打开wandb的个人project即可看到相应的图表记录。<br><img src="/img/wandb/4.jpg" alt="主要几种图表"><br><img src="/img/wandb/5.jpg" alt="valid数据"><br><strong>具体每个图表信息或者添加还需要进一步了解</strong></p><h2 id="图表对比"><a href="#图表对比" class="headerlink" title="图表对比"></a>图表对比</h2><p>主要目的：以train/loss为例对比不同参数或者模型下的loss曲线变化。<br><img src="/img/wandb/6.jpg" alt="两个正在运行的project，一个已经运行的project"><br>随便打开一个project的train/loss图表，点击右上角的三个点，选择<code>Add to report</code>。<br><img src="/img/wandb/7.jpg" alt="添加report"><br><img src="/img/wandb/8.jpg" alt="设置下名字"><br><strong>在这个report中就可以添加想要对比的曲线，曲线可以是正在运行的模型，也可以是已经运行完的模型</strong><br><img src="/img/wandb/9.jpg" alt="加号可以添加project，在project中进行切换，可以设置颜色和可见情况"><br><img src="/img/wandb/11.jpg" alt="之后点击保存即可看到对比图"></p>]]></content>
      
      
      
        <tags>
            
            <tag> fairseq </tag>
            
            <tag> 小助手 </tag>
            
            <tag> wandb </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vscode+win10远程调试linux服务器python项目</title>
      <link href="/2022/09/02/vscode_remote_dubug/"/>
      <url>/2022/09/02/vscode_remote_dubug/</url>
      
        <content type="html"><![CDATA[<h2 id="vscode远程连接linux服务器"><a href="#vscode远程连接linux服务器" class="headerlink" title="vscode远程连接linux服务器"></a>vscode远程连接linux服务器</h2><p>这部分较为简单，大部分网上都有教程，<a href="https://mp.weixin.qq.com/s/s5IhxV2ooX3JN_X416nidA">推荐看这个</a>。连接上后vscode左下角有SSH连接标志。<br><img src="/img/vscode_remote_debug_md/1.jpg" alt="左下角截图"></p><p>接下来就可以打开相应的python项目了。<br><img src="/img/vscode_remote_debug_md/2.jpg" alt="打开fairseq项目文件夹"></p><h2 id="命令行参数设置"><a href="#命令行参数设置" class="headerlink" title="命令行参数设置"></a>命令行参数设置</h2><p>以fair-generate为例，命令行如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fairseq-generate data-bin/iwslt14.tokenized.de-en \</span><br><span class="line">    --path checkpoints/checkpoint_best.pt \</span><br><span class="line">    --batch-size 128 --beam 5 --remove-bpe</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>找到该文件夹下的.vscode文件，打开其中的launch.json。<br><img src="/img/vscode_remote_debug_md/3.jpg" alt="找到该文件"><br>在vscode中的launch.json文件中添加args参数，具体如下图所示。<br><img src="/img/vscode_remote_debug_md/4.jpg" alt="命令行参数格式"><br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="comment">// 使用 IntelliSense 了解相关属性。 </span></span><br><span class="line">    <span class="comment">// 悬停以查看现有属性的描述。</span></span><br><span class="line">    <span class="comment">// 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387</span></span><br><span class="line">    <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.2.0&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;configurations&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Python: Current File&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;python&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;request&quot;</span><span class="punctuation">:</span> <span class="string">&quot;launch&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;program&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;file&#125;&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;console&quot;</span><span class="punctuation">:</span> <span class="string">&quot;integratedTerminal&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;justMyCode&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;args&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="string">&quot;data-bin/iwslt14.tokenized.de-en&quot;</span><span class="punctuation">,</span><span class="string">&quot;--path&quot;</span><span class="punctuation">,</span><span class="string">&quot;checkpoints/checkpoint_best.pt&quot;</span><span class="punctuation">,</span><span class="string">&quot;--batch-size&quot;</span><span class="punctuation">,</span><span class="string">&quot;10&quot;</span><span class="punctuation">,</span><span class="string">&quot;--beam&quot;</span><span class="punctuation">,</span><span class="string">&quot;5&quot;</span><span class="punctuation">,</span><span class="string">&quot;--remove-bpe&quot;</span><span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></p><p><strong>相较于命令行参数没有了fairseq-generate，转而到generate.py文件下调试。</strong></p><h2 id="选择环境"><a href="#选择环境" class="headerlink" title="选择环境"></a>选择环境</h2><p>由于python项目可能要在相应的anaconda环境中才能运行，因此需要先选择环境，在vscode中按住<code>ctrl+shift+P</code>调出这个控制界面。<br><img src="/img/vscode_remote_debug_md/5.jpg" alt="选择解释器"><br><img src="/img/vscode_remote_debug_md/6.jpg" alt="选择py3_7环境（之前已经创建好）"></p><h2 id="开始调试"><a href="#开始调试" class="headerlink" title="开始调试"></a>开始调试</h2><p><strong>在generate.py文件下调试。</strong>，设置好断点，在菜单选择<strong>运行-&gt;调试</strong>。<br><img src="/img/vscode_remote_debug_md/7.jpg" alt="就可以看到变量等情况"></p>]]></content>
      
      
      
        <tags>
            
            <tag> vscode </tag>
            
            <tag> 小助手 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fairseq的使用入门</title>
      <link href="/2022/08/29/fairseq/"/>
      <url>/2022/08/29/fairseq/</url>
      
        <content type="html"><![CDATA[<h2 id="fairseq常用命令"><a href="#fairseq常用命令" class="headerlink" title="fairseq常用命令"></a>fairseq常用命令</h2><h3 id="通用命名参数"><a href="#通用命名参数" class="headerlink" title="通用命名参数"></a>通用命名参数</h3><div class="table-container"><table><thead><tr><th style="text-align:left">命令</th><th style="text-align:left">参数</th><th style="text-align:left">用法</th><th style="text-align:left">可选项/备注</th></tr></thead><tbody><tr><td style="text-align:left">通用命名参数</td><td style="text-align:left"><code>--no-progress-bar</code></td><td style="text-align:left">关闭进度条</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--log-interval</code></td><td style="text-align:left">每N批记录进度(禁用进度条时)</td><td style="text-align:left">默认：100</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--log-format</code></td><td style="text-align:left">选择日志格式</td><td style="text-align:left">选项：json,none,simple,tqdm</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--log-file</code></td><td style="text-align:left">指定输出metric的日志文件</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--tensorboard-logdir</code></td><td style="text-align:left">tensorboard的日志文件位置</td><td style="text-align:left">须和-–logdir of running tensorboard相同，默认无日志</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--wandb-project</code></td><td style="text-align:left">用于日志的权重和偏置项目名</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--seed</code></td><td style="text-align:left">设置随机数种子</td><td style="text-align:left">默认为1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--cpu</code></td><td style="text-align:left">使用CPU而不是CUDA</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--tpu</code></td><td style="text-align:left">使用TPU而不是CUDA</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--bf16</code></td><td style="text-align:left">使用数据格式bfloat16,前提—tpu</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--memory-efficient-bf16</code></td><td style="text-align:left">使用内存高效的bfloat16,前提—bf16</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--fp16</code></td><td style="text-align:left">使用数据格式FP16</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--memory-efficient-fp16</code></td><td style="text-align:left">使用数据格式内存高效的FP16,</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--fp16-no-flatten-grads</code></td><td style="text-align:left">不展开FP16梯度tensor</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--fp16-init-scale</code></td><td style="text-align:left"></td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--on-cpu-convert-precision</code></td><td style="text-align:left">浮点数转化为fp16/bf16将在CPU上完成.</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--amp</code></td><td style="text-align:left">使用自动混合精度</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--usr-dir</code></td><td style="text-align:left">包含个性扩展的模块路径（tasks/architectures）</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--empty-cache-freq</code></td><td style="text-align:left">pytorch CUDA缓存清理频率</td><td style="text-align:left">默认：0（不清理）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--model-parallel-size</code></td><td style="text-align:left">使用并行的GPU数量</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--quantization-config-path</code></td><td style="text-align:left">quantization配置文件路径</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--criterion</code></td><td style="text-align:left">损失函数</td><td style="text-align:left">默认：交叉熵</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--tokenizer</code></td><td style="text-align:left">分词工具</td><td style="text-align:left">可选：mosses,nltk,space</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--bpe</code></td><td style="text-align:left">bpe分词</td><td style="text-align:left">可选：byte_bpe, bytes, characters, fastbpe, gpt2, bert, hf_byte_bpe, sentencepiece, subword_nmt</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--optimizer</code></td><td style="text-align:left">优化器</td><td style="text-align:left">可选：adadelta, adafactor, adagrad, adam, adamax, composite, cpu_adam, lamb, nag, sgd</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--lr-scheduler</code></td><td style="text-align:left">学习率管理</td><td style="text-align:left">可选：cosine, fixed, inverse_sqrt, manual, pass_through, polynomial_decay, reduce_lr_on_plateau, step, tri_stage, triangular，默认：fixed</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--scoring</code></td><td style="text-align:left">评价指标</td><td style="text-align:left">可选：bert_score, sacrebleu, bleu, chrf, meteor, wer，默认：bleu</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--task</code></td><td style="text-align:left">任务选项</td><td style="text-align:left">可选：multilingual_language_modeling, speech_unit_modeling, hubert_pretraining, translation等，默认：translation</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--dataset-impl</code></td><td style="text-align:left">处理数据集方式</td><td style="text-align:left">可选：raw, lazy, cached, mmap, fasta, huffman，默认：mmap</td></tr></tbody></table></div><p>命名参数省略：</p><ul><li>aim-repo</li><li>aim-run-hash</li><li>azureml-logging</li><li>fp16-init-scale</li><li>fp16-scale-window</li><li>fp16-scale-tolerance</li><li>min-loss-scale</li><li>threshold-loss-scale</li><li>amp-batch-retries</li><li>amp-init-scale</li><li>amp-scale-window</li><li>all-gather-list-size</li><li>profile</li><li>reset-logging</li><li>suppress-crashes</li><li>use-plasma-view</li><li>plasma-path</li></ul><h3 id="fairseq-preprocess参数"><a href="#fairseq-preprocess参数" class="headerlink" title="fairseq-preprocess参数"></a>fairseq-preprocess参数</h3><div class="table-container"><table><thead><tr><th style="text-align:left">命令</th><th style="text-align:left">参数</th><th style="text-align:left">用法</th><th style="text-align:left">可选项/备注</th></tr></thead><tbody><tr><td style="text-align:left">fairseq-preprocess</td><td style="text-align:left"></td><td style="text-align:left">数据预处理：建词典并将训练数据二进制化</td><td style="text-align:left">以下为预处理命令</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>-s, --source-lang</code></td><td style="text-align:left">源语言</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>-t, --target-lang</code></td><td style="text-align:left">目标语言</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--trainpref</code></td><td style="text-align:left">两个语言的训练文件前缀</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--validpref</code></td><td style="text-align:left">两个语言的验证文件前缀</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--testpref</code></td><td style="text-align:left">两个语言的测试文件前缀</td><td style="text-align:left">说明：例如我们的数据中只有训练数据和测试数据，且文件后缀为src和tgt，即train.src、train.tgt、test.src和test.tgt，那么通过指定—source-lang src —target-lang tgt —trainpref train —testpref test，也可以读取的对应的文件。</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--align-suffix</code></td><td style="text-align:left">对齐后缀</td><td style="text-align:left">未知</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--destdir</code></td><td style="text-align:left">输出文件位置</td><td style="text-align:left">默认：data-bin</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--thresholdtgt</code></td><td style="text-align:left">将目标语言出现次数少于阈值的词映射到unkown</td><td style="text-align:left">默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--thresholdsrc</code></td><td style="text-align:left">将源语言出现次数少于阈值的词映射到unkown</td><td style="text-align:left">默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--tgtdict</code></td><td style="text-align:left">重复使用给定的目标语言字典</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--srcdict</code></td><td style="text-align:left">重复使用给定的源语言字典</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--nwordstgt</code></td><td style="text-align:left">目标语言中需要retain的词数量，(解决oov问题）</td><td style="text-align:left">默认：-1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--nwordssrc</code></td><td style="text-align:left">源语言中需要retain的词数量</td><td style="text-align:left">默认：-1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--alignfile</code></td><td style="text-align:left">对齐文件（可选）</td><td style="text-align:left">翻译 文字一一对齐</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--joined-dictionary</code></td><td style="text-align:left">生成联合的字典</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--only-source</code></td><td style="text-align:left">只处理源语言</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--padding-factor</code></td><td style="text-align:left">填充字典数量为N的倍数</td><td style="text-align:left">默认：8</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--workers</code></td><td style="text-align:left">并行进程数量</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--dict-only</code></td><td style="text-align:left">只构建字典</td><td style="text-align:left">默认：False</td></tr></tbody></table></div><h3 id="fairseq-train参数"><a href="#fairseq-train参数" class="headerlink" title="fairseq-train参数"></a>fairseq-train参数</h3><div class="table-container"><table><thead><tr><th style="text-align:left">命令</th><th style="text-align:left">参数</th><th style="text-align:left">用法</th><th style="text-align:left">可选项/备注</th></tr></thead><tbody><tr><td style="text-align:left">fairseq-train</td><td style="text-align:left"></td><td style="text-align:left">在一个或多个GPU上训练模型</td><td style="text-align:left">以下为训练命令</td></tr><tr><td style="text-align:left">数据集加载</td><td style="text-align:left"><code>--num-workers</code></td><td style="text-align:left">加载数据的并行进程数量</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--skip-invalid-size-inputs-valid-test</code></td><td style="text-align:left">忽略验证集和测试集中太长或太短的句子</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--max-token</code></td><td style="text-align:left">一个batch中tokens的最大数量</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--batch-size,--max-sentences</code></td><td style="text-align:left">batch size</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--required-batch-size-multiple</code></td><td style="text-align:left">要求batchsize是该值的倍数</td><td style="text-align:left">默认：8</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--required-seq-len-multiple</code></td><td style="text-align:left">要求最大序列长度是该值的倍数</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--data-buffer-size</code></td><td style="text-align:left">预先加载的batch数量</td><td style="text-align:left">default：10</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--train-subset</code></td><td style="text-align:left">数据集中用于训练的子集</td><td style="text-align:left">例如：trian,valid,test。默认：train</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--valid-subset</code></td><td style="text-align:left">数据集中用于验证的子集</td><td style="text-align:left">例如：trian,valid,test。默认：valid</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--ignore-unused-valid-subsets</code></td><td style="text-align:left">do not raise error if valid subsets are ignored</td><td style="text-align:left">默认：False（未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--validate-interval</code></td><td style="text-align:left">每N个epochs在验证集上验证</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--validate-interval-updates</code></td><td style="text-align:left">每N个updates在验证集上验证</td><td style="text-align:left">默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--validate-after-updates</code></td><td style="text-align:left">在N次更新后才验证</td><td style="text-align:left">默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--fixed-validation-seed</code></td><td style="text-align:left">设置验证时的种子值</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--disable-validation</code></td><td style="text-align:left">取消验证</td><td style="text-align:left">default:False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--max-tokens-valid</code></td><td style="text-align:left">验证时每批次的最大tokens数</td><td style="text-align:left">默认和 <code>--max-tokens</code>相同</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--batch-size-valid</code></td><td style="text-align:left">验证时batchsize</td><td style="text-align:left">默认和<code>--batch-size</code>相同</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--max-valid-steps，--nval</code></td><td style="text-align:left">评估的总batch数量</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--curriculum</code></td><td style="text-align:left">在前N个epochs中不打乱batch</td><td style="text-align:left">N默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--gen-subset</code></td><td style="text-align:left">选择生成的数据集子集（train,valid,text）</td><td style="text-align:left">默认：test</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--num-shards</code></td><td style="text-align:left">N文件切片</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--shard-id</code></td><td style="text-align:left">生成shards的其中第i个（i&lt;num-shards）</td><td style="text-align:left">默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--grouped-shuffling</code></td><td style="text-align:left">在num_shards组中打乱批次，使得每个子进程中的序列长度相似，前提batches按照长度排序</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--update-epoch-batch-itr</code></td><td style="text-align:left"></td><td style="text-align:left">默认：False （未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--update-ordered-indices-seed</code></td><td style="text-align:left"></td><td style="text-align:left">默认：False （未知）</td></tr><tr><td style="text-align:left">分布式训练</td><td style="text-align:left"><code>--distributed-world-size</code></td><td style="text-align:left">设置所有节点的GPU总数，等于可见GPU数量</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--distributed-num-procs</code></td><td style="text-align:left">fork的子进程数量，等于可见GPU数量</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--distributed-rank</code></td><td style="text-align:left">目前进程的级别</td><td style="text-align:left">默认：0 (未知)</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--distributed-backend</code></td><td style="text-align:left">分布式后端</td><td style="text-align:left">默认：nccl (未知)</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--distributed-init-method</code></td><td style="text-align:left">初始化方法</td><td style="text-align:left">(未知)</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--distributed-port</code></td><td style="text-align:left">端口号，如果设置了上一条就不用管</td><td style="text-align:left">默认：-1(未知)</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--device-id,--local-rank</code></td><td style="text-align:left">使用哪一个GPU</td><td style="text-align:left">默认：0(未知)</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--distributed-no-spawn</code></td><td style="text-align:left">即使有多个GPU也不生成多个进程</td><td style="text-align:left">默认：False(未知)</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--ddp-backend</code></td><td style="text-align:left">DistributedDataParallel后端</td><td style="text-align:left">默认：pytorch_ddp</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--fix-batches-to-gpus</code></td><td style="text-align:left">不要在gpu之间切换批次;这减少了整体的随机性，可能会影响精度，但避免了重新读取数据的成本</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--find-unused-parameters</code></td><td style="text-align:left">控制未使用参数检测</td><td style="text-align:left">默认：False （未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--gradient-as-bucket-view</code></td><td style="text-align:left"></td><td style="text-align:left">默认：False （未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--heartbeat-timeout</code></td><td style="text-align:left">如果N秒内进程没反应就Kill，N为-1的话不kill</td><td style="text-align:left">默认：-1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--broadcast-buffers</code></td><td style="text-align:left">在GPU之间复制不可训练的参数</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--slowmo-momentum</code></td><td style="text-align:left">slowMo动量设置</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--slowmo-base-algorithm</code></td><td style="text-align:left">基础算法sgd或localsgd</td><td style="text-align:left">默认:localsgd</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--localsgd-frequency</code></td><td style="text-align:left">localsgd的allreduce频率</td><td style="text-align:left">默认:3</td></tr><tr><td style="text-align:left">部分其他参数见文档</td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left"></td></tr><tr><td style="text-align:left">模型配置</td><td style="text-align:left"><code>--arch -a</code></td><td style="text-align:left">模型架构选择</td><td style="text-align:left">可选项项:transformer_tiny,transformer,transformer_iwslt_de_en, transformer_wmt_en_de,roberta_base…</td></tr><tr><td style="text-align:left">优化设置</td><td style="text-align:left"><code>--max-epoch</code></td><td style="text-align:left">到特定的epoch时停止训练</td><td style="text-align:left">默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--max-upadte</code></td><td style="text-align:left">到特定的update时停止训练</td><td style="text-align:left">默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--stop-time-hours</code></td><td style="text-align:left">到指定的小时数后停止训练</td><td style="text-align:left">默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--clip-norm</code></td><td style="text-align:left">梯度剪切</td><td style="text-align:left">默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--sentence-avg</code></td><td style="text-align:left">使用一批次的句子数量正则化梯度（而不是tokens数量）</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--update-freq</code></td><td style="text-align:left">在第i个epoch时按照每Ni个批次更新参数</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--lr</code></td><td style="text-align:left">在前N个epoch设置学习率，使用LR_N的话epochs&gt;N</td><td style="text-align:left">默认：0.25 （未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--stop-min-lr</code></td><td style="text-align:left">当学习率降到该值时停止训练，默认不停止</td><td style="text-align:left">默认：-1.0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--use-bmuf</code></td><td style="text-align:left">使用全局优化器来同步多GPU/shards上的模型</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--skip-remainder-batch</code></td><td style="text-align:left">如果设置这个选项，则会跳过最后一批不满的批次</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left">检查点</td><td style="text-align:left"><code>--save-dir</code></td><td style="text-align:left">保存检查点的路径</td><td style="text-align:left">默认：checkpoints</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--restore-file</code></td><td style="text-align:left">加载已保存检查点文件的路径</td><td style="text-align:left">默认：checkpoints</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--continue-once</code></td><td style="text-align:left">从该检查点继续，除非设置了<code>--restore-file</code></td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--finetune-from-model</code></td><td style="text-align:left">从预训练模型微调</td><td style="text-align:left">未知</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--reset-dataloader</code></td><td style="text-align:left">如果设置了，就不从checkpoints中加载dataloader state</td><td style="text-align:left">默认：False (未知)</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--reset-lr-sheduler</code></td><td style="text-align:left">如果设置了，就不从checkpoints中加载lr sheduler state</td><td style="text-align:left">默认：False (未知)</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--reset-meters</code></td><td style="text-align:left">如果设置了，就不从checkpoints中加载meters</td><td style="text-align:left">默认：False (未知)</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--reset-optimizer</code></td><td style="text-align:left">如果设置了，就不从checkpoints中加载optimizer state</td><td style="text-align:left">默认：False (未知)</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--save-interval</code></td><td style="text-align:left">每N个epoch保存checkpoint</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--save-interval-updates</code></td><td style="text-align:left">每N个epoch验证并保存checkpoint</td><td style="text-align:left">默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--keep-interval-updates</code></td><td style="text-align:left">保存上一个选项的最后N个checkpoints</td><td style="text-align:left">默认：-1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--keep-interval-updates-pattern</code></td><td style="text-align:left">和上一条同时使用，不删除第k个checkpoints，其中k%keep_interval_updates_pattern==0</td><td style="text-align:left">默认：-1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--keep-last-epochs</code></td><td style="text-align:left">保存最后N个epoch的checkpoints</td><td style="text-align:left">默认：-1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--keep-best-epochs</code></td><td style="text-align:left">保存N个最佳sorce的checkpoints</td><td style="text-align:left">默认：-1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--no-save</code></td><td style="text-align:left">不保存model和checkpoints</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--no-epoch-checkpoints</code></td><td style="text-align:left">只保存最后一个epoch和最佳的checkpoints</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--no-last-checkpoints</code></td><td style="text-align:left">不保存最后一个epoch checkpoints</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--no-save-optimizer-state</code></td><td style="text-align:left">不将optimizer state视为checkpoint的一部分</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--best-checkpoint-metric</code></td><td style="text-align:left">best checkpoint的标准</td><td style="text-align:left">默认：loss （<code>重要</code>）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--maximize-best-checkpoint-metric</code></td><td style="text-align:left">选择最大的metric value来保存最佳的checkpoints</td><td style="text-align:left">默认：False（未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--patience</code></td><td style="text-align:left">连续的N次验证性能没有提升则停止训练，会被<code>–validate-interval</code>影响</td><td style="text-align:left">默认：-1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--checkpoint-suffix</code></td><td style="text-align:left">检查点文件的后缀</td><td style="text-align:left">默认：空</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--checkpoint-shard-count</code></td><td style="text-align:left">如果checkpoints文件超过300GB,会将其切片防止存储不足。</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--load-checkpoint-on-all-dp-ranks</code></td><td style="text-align:left">在所有并行设备上加载checkpoints，从第一个</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--write-checkpoints-asynchronously, --save-async</code></td><td style="text-align:left">异步保存检查点，功能测试中</td><td style="text-align:left">默认：False</td></tr></tbody></table></div><h3 id="fairseq-generate参数"><a href="#fairseq-generate参数" class="headerlink" title="fairseq-generate参数"></a>fairseq-generate参数</h3><div class="table-container"><table><thead><tr><th style="text-align:left">命令</th><th style="text-align:left">参数</th><th style="text-align:left">用法</th><th style="text-align:left">可选项/备注</th></tr></thead><tbody><tr><td style="text-align:left">fairseq-generate</td><td style="text-align:left">使用已训练的模型翻译预先处理好的数据</td><td style="text-align:left"></td><td style="text-align:left"></td></tr><tr><td style="text-align:left">Generation</td><td style="text-align:left"><code>--path</code></td><td style="text-align:left">模型文件路径，冒号间隔</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--post-process，--remove-bpe</code></td><td style="text-align:left">去除BPE、字母分割等</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--quiet</code></td><td style="text-align:left">只输出最后的分数</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--model-overrides</code></td><td style="text-align:left">在生成时重载模型参数，替换训练时的参数</td><td style="text-align:left">默认：无</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--results-path</code></td><td style="text-align:left">保存评价结果的路径</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--beam</code></td><td style="text-align:left">beam size</td><td style="text-align:left">默认：5</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--nbest</code></td><td style="text-align:left">输出翻译的结果数</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--max-len-a</code></td><td style="text-align:left">生成句子最大长度ax+b,x是源句子长度</td><td style="text-align:left">同下使用，默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--max-len-b</code></td><td style="text-align:left">生成句子最大长度ax+b,x是源句子长度</td><td style="text-align:left">同上使用，默认：200</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--min-len</code></td><td style="text-align:left">最小的生成长度</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--match-source-len</code></td><td style="text-align:left">生成的句子长度和原句子长度相同</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--unnormalized</code></td><td style="text-align:left">compare unnormalized hypothesis scores</td><td style="text-align:left">默认：False（未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--no-beamable-mm</code></td><td style="text-align:left">在attention layers中不使用BeamableMM</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--lenpen</code></td><td style="text-align:left">长度惩罚，如果小于1.0倾向短句子，大于1.0则倾向长句子</td><td style="text-align:left">默认：1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--unkpen</code></td><td style="text-align:left">未知单词惩罚，如果小于0产生更多unks，大于0产生更少</td><td style="text-align:left">默认：0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--replace-unk</code></td><td style="text-align:left">替换unk，使用字典</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--sacrebleu</code></td><td style="text-align:left">使用sacrebleu</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--score-reference</code></td><td style="text-align:left">只给参考翻译打分</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--prefix-size</code></td><td style="text-align:left">给定前缀长度来初始化生成</td><td style="text-align:left">默认：0（未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--no-repeat-ngram-size</code></td><td style="text-align:left">ngram中不能重复生成</td><td style="text-align:left">默认：0（未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--sampling</code></td><td style="text-align:left">在假设中采样而不是使用beam search</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--sampling-topk</code></td><td style="text-align:left">从K个可能的下个字中采样而不是从全部词中采样</td><td style="text-align:left">默认：-1</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--constraints</code></td><td style="text-align:left">词法约束解码过程</td><td style="text-align:left">可选：ordered,unordered（未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--temperature</code></td><td style="text-align:left">temperature for generation</td><td style="text-align:left">默认：1.0 （未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--diverse-beam-groups</code></td><td style="text-align:left">组数</td><td style="text-align:left">默认：-1（未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--diverse-beam-strength</code></td><td style="text-align:left">strength of diversity penalty for Diverse Beam Search</td><td style="text-align:left">默认：-1（未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--diverse-rate</code></td><td style="text-align:left">strength of diversity penalty for Diverse Siblings Search</td><td style="text-align:left">默认：-1（未知）</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--print-step</code></td><td style="text-align:left">输出steps</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--decoding-format</code></td><td style="text-align:left">解码格式</td><td style="text-align:left">可选：unigram, ensemble, vote, dp, bs</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--no-seed-provided</code></td><td style="text-align:left">设置了，生成时就不初始化种子</td><td style="text-align:left">默认：False</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"><code>--eos-token</code></td><td style="text-align:left">设置停止的token</td></tr></tbody></table></div><p>generation省略：</p><ul><li>print-alignment</li><li>lm-path</li><li>lm-weight</li><li>iter-decode-eos-penalty</li><li>iter-decode-max-iter</li><li>iter-decode-force-max-iter</li><li>iter-decode-with-beam</li><li>iter-decode-with-external-reranker</li></ul><h2 id="fairseq可扩展部分"><a href="#fairseq可扩展部分" class="headerlink" title="fairseq可扩展部分"></a>fairseq可扩展部分</h2><h3 id="Tasks"><a href="#Tasks" class="headerlink" title="Tasks"></a>Tasks</h3><p>Tasks存储字典，并且对加载/迭代数据集提供帮助，初始化Model/Criterion，计算损失。用法示例如下：<br><img src="/img/fairseq_md/1.jpg" alt="用例"></p><div class="table-container"><table><thead><tr><th style="text-align:left">命令</th><th style="text-align:left">参数</th><th style="text-align:left">用法</th><th style="text-align:left">可选项/备注</th></tr></thead><tbody><tr><td style="text-align:left">Additional command-line arguments</td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left">待续未完</td></tr></tbody></table></div><h3 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h3><p>该部分定义网络前馈过程，封装可学习参数，可以设置模型架构<br><img src="/img/fairseq_md/3.jpg" alt="Additional command-line arguments"></p><h3 id="Criterions"><a href="#Criterions" class="headerlink" title="Criterions"></a>Criterions</h3><p>给定model和batch，计算损失函数</p><h3 id="Optimizers"><a href="#Optimizers" class="headerlink" title="Optimizers"></a>Optimizers</h3><p>根据梯度更新模型参数</p><h3 id="学习率管理"><a href="#学习率管理" class="headerlink" title="学习率管理"></a>学习率管理</h3><h3 id="数据加载和处理"><a href="#数据加载和处理" class="headerlink" title="数据加载和处理"></a>数据加载和处理</h3><h3 id="模型模块"><a href="#模型模块" class="headerlink" title="模型模块"></a>模型模块</h3><h2 id="fairseq-translation使用"><a href="#fairseq-translation使用" class="headerlink" title="fairseq-translation使用"></a>fairseq-translation使用</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Download and prepare the data</span></span><br><span class="line"><span class="built_in">cd</span> examples/translation/</span><br><span class="line">bash prepare-iwslt14.sh</span><br><span class="line"><span class="built_in">cd</span> ../..</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocess/binarize the data</span></span><br><span class="line">TEXT=examples/translation/iwslt14.tokenized.de-en</span><br><span class="line">fairseq-preprocess --source-lang de --target-lang en \</span><br><span class="line">    --trainpref <span class="variable">$TEXT</span>/train --validpref <span class="variable">$TEXT</span>/valid --testpref <span class="variable">$TEXT</span>/test \</span><br><span class="line">    --destdir data-bin/iwslt14.tokenized.de-en \</span><br><span class="line">    --workers 20</span><br></pre></td></tr></table></figure><p><img src="/img/fairseq_md/5.jpg" alt="命令行截图"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0 fairseq-train \</span><br><span class="line">    data-bin/iwslt14.tokenized.de-en \</span><br><span class="line">    --<span class="built_in">arch</span> transformer_iwslt_de_en --share-decoder-input-output-embed \</span><br><span class="line">    --optimizer adam --adam-betas <span class="string">&#x27;(0.9, 0.98)&#x27;</span> --clip-norm 0.0 \</span><br><span class="line">    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \</span><br><span class="line">    --dropout 0.3 --weight-decay 0.0001 \</span><br><span class="line">    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \</span><br><span class="line">    --max-tokens 4096 \</span><br><span class="line">    --eval-bleu \</span><br><span class="line">    --eval-bleu-args <span class="string">&#x27;&#123;&quot;beam&quot;: 5, &quot;max_len_a&quot;: 1.2, &quot;max_len_b&quot;: 10&#125;&#x27;</span> \</span><br><span class="line">    --eval-bleu-detok moses \</span><br><span class="line">    --eval-bleu-remove-bpe \</span><br><span class="line">    --eval-bleu-print-samples \</span><br><span class="line">    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric</span><br></pre></td></tr></table></figure><p><img src="/img/fairseq_md/6.jpg" alt="训练截图"><br><img src="/img/fairseq_md/7.jpg" alt="训练159个epoch截图"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fairseq-generate data-bin/iwslt14.tokenized.de-en \</span><br><span class="line">    --path checkpoints/checkpoint_best.pt \</span><br><span class="line">    --batch-size 128 --beam 5 --remove-bpe</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/img/fairseq_md/8.jpg" alt="在测试集上验证截图"></p>]]></content>
      
      
      
        <tags>
            
            <tag> fairseq </tag>
            
            <tag> 机器翻译 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
